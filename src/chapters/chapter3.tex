\newpage
\section*{Exploration Questions}
\addcontentsline{toc}{section}{Exploration Questions}



\section{Matrices}
	Talking about matrices now may seem like a non sequitur, but we will soon be
	using them as a notational device.
	\begin{definition}[Matrix]
		An $m\times n$ \emph{matrix}\index{matrix} is a rectangular
		array of numbers with $m$ rows and $n$ columns, usually surrounded by
		brackets.
	\end{definition}

	The \emph{dimensions}, \emph{shape}, or \emph{size} of a matrix refers
	to the number of rows and columns in a matrix and is always listed
	as $\#$rows and then $\#$columns. The most common way to specify the
	size of a matrix is by writing ``$\text{rows}\times\text{columns}$''.

	We are already familiar with certain matrices. When we write down
	a column vector like $\mat{2\\1}$, we are writing down a $2\times 1$ matrix.
	It is a theorem that a vector $\vec v\in\R^n$ can be represented by an $n\times 1$
	matrix.

	We can also \emph{index} a matrix---that is, refer to particular entries in the matrix.
	An $m\times n$ matrix $A$ takes the form
	\[
		A = \matc{
			a_{11}&a_{12}&a_{13}&\cdots& a_{1n}\\
			a_{21}&a_{22}&a_{23}&\cdots& a_{2n}\\
			a_{31}&a_{32}&a_{33}&\cdots& a_{3n}\\
			\vdots&\vdots&\vdots&\ddots&\vdots\\
			a_{m1}&a_{m2}&a_{m3}&\cdots& a_{mn}\\
		}.
	\]
	Here, $a_{ij}$ refers to the number in the $i$th row and $j$th column of $A$\footnote{
	It would be clearer to write $a_{i,j}$, but it is tradition to omit the comma.}. We can
	use \emph{index notation}\index{matrix indexing} to define a matrix. For example,
	we can define a $2\times 3$ matrix $B=[b_{ij}]$ where $b_{ij}=i+j$.  In this case,
	\[
		B=[b_{ij}] = \mat{2&3&4\\3&4&5}.
	\]
	Writing $B=[b_{ij}]$ is shorthand for ``$B$ is the matrix whose $i,j$ entry is $b_{ij}$''.

	\bigskip
	Matrices have no intrinsic meaning---\emph{they are just boxes of numbers}.
	But, we can use matrices to represent things like vectors, coefficients of
	equations, grocery lists, etc..

\subsection{Special Matrices}
	There are two special matrices that will come up often.
	\begin{definition}[The Identity Matrix]
		The $n\times n$ \emph{identity matrix}\index{identity matrix}, written $I_{n\times n}$ is the $n\times n$
		matrix with ones along the diagonal and zeros everywhere else.
	\end{definition}
	Some examples are
	\[
		I_{2\times 2}=\mat{1&0\\0&1}\qquad\text{and}\qquad I_{3\times 3}=\mat{1&0&0\\0&1&0\\0&0&1}.
	\]
	Identity matrices are always square, and when it is obvious from context what
	the size must be, we omit the subscript and simply write $I$.

	\begin{definition}[The Zero Matrix]
		The $m\times n$ \emph{zero matrix}\index{zero matrix},
		written $0_{m\times n}$ is the matrix of all zeros.
	\end{definition}
	Some examples are
	\[
		0_{2\times 2}=\mat{0&0\\0&0}\qquad\text{and}\qquad
		0_{3\times 2}=\mat{0&0\\0&0\\0&0}.
	\]
	Again, when the size of a zero matrix is obvious from context, we omit the subscript
	and simply write $0$.

\subsection{Block Notation}
	Occasionally we want to create new matrices by stacking existing matrices.
	These are called \emph{block} matrices. Typically, but not always, we draw lines
	to emphasize there is something special about the entry of a block matrix.

	For example, if $M=\mat{1&2\\3&4}$, then
	\[
		\begin{bmatrix}[c|c]
			0_{2\times 2} & I_{2\times 2}\\
			\hline
			I_{2\times 2} & M\\
		\end{bmatrix}
		=\mat{0&0&1&0\\0&0&0&1\\
		1&0&1&2\\
		0&1&3&4}.
	\]
	We can also read block notation backwards to define a matrix. For example,
	we can write
	\[
		\begin{bmatrix}[c|c]
			7 & \vec v\\
			\hline
			\vec u & M\\
		\end{bmatrix}
		=\mat{7&0&1&0\\0&0&0&1\\
		1&0&1&2\\
		0&1&3&4},
	\]
	which defines the matrix $M=\mat{0&0&1\\0&1&2\\1&3&4}$, the column 
	vector $\vec u=\mat{0\\1\\0}$
	and the row vector $\vec v=\mat{0&1&0}$.

\section{Systems of Linear Equations}

	Consider the vector equation
	\begin{equation}\label{EQVECEQ}
		t\vec u+s\vec v+r\vec w = \vec p\qquad\text{where}\qquad \vec u=\mat{1\\2\\1},\ 
		\vec v=\mat{2\\1\\-4},\ \vec w=\mat{-2\\-5\\1},\ \vec p=\mat{-15\\-21\\18}.
	\end{equation}
	A \emph{solution} to this equation is values of $t$, $s$, and $r$ that make the equation true.
	There are many ways to find $t$, $s$, and $r$, but one way that always works is by
	equating components of each vector in the equation. By equating components in equation
	\eqref{EQVECEQ}, we get the following system:
	\begin{equation}
		\label{EQVECEQ2}
		\systeme[tsr]{
			t+2s-2r=-15@\qquad\text{row}_1,
			2t+s-5r=-21@\qquad\text{row}_2,
			t-4s+r=18@\qquad\text{row}_3
		}
	\end{equation}

	The system \eqref{EQVECEQ2} could be solve by \emph{substitution}:
	solve the first equation for $t$; substitute $t$ into the second two equation
	which then would contain $s$ and $r$ as the only unknowns; solve the second
	equation for $s$; substitute $s$ into the last equation which now contains
	$r$ as the only unknown; solve for $r$; work backwards plugging in $r$ to get $s$,
	and finally plugging in $r$ and $s$ to get $t$.

	We will instead solve system \eqref{EQVECEQ2} by \emph{elimination}\footnote{
	Elimination is sometimes referred to as \emph{Gaussian elimination}
	or \emph{Gauss-Jordan elimination}.}.  Observe the following: if $A=B$ and
	$C=D$, then $A+ \alpha C=B+\alpha D$ for any $\alpha$.
	Using this fact, we can eliminate unknowns by summing equations rather than by
	substituting.
	\begin{align*}
	\sysdelim\{.
		\systeme[tsr]{
			t+2s-2r=-15,
			2t+s-5r=-21,
			t-4s+r=18
		}
		&\xrightarrow{\text{row}_3\mapsto\text{row}_3-\text{row}_1}
	\sysdelim\{.
		\systeme[tsr]{
			t+2s-2r=-15,
			2t+s-5r=-21,
			-6s+3r=33
		}\\[4pt]
		&\xrightarrow{\text{row}_2\mapsto\text{row}_2-2\text{row}_1}
	\sysdelim\{.
		\systeme[tsr]{
			t+2s-2r=-15,
			-3s-r=9,
			-6s+3r=33
		}\\[4pt]
		&\xrightarrow{\text{row}_3\mapsto\text{row}_3-2\text{row}_3}
	\sysdelim\{.
		\systeme[tsr]{
			t+2s-2r=-15,
			-3s-r=9,
			  5r=15
		}\\[4pt]
	\end{align*}
	At this point, we have eliminated all but one unknown from
	the last equation. By inspection, we see that $r=3$; substituting 
	$r$ into the second equation gives $s=-4$; finally, substituting both
	$r$ and $s$ into the first equation gives $t=-1$.

	The benefit of elimination over substitution is that elimination
	is \emph{algorithmic} (that is, you could program a computer to do it)
	and can be made notationally convenient.

\subsection{Row Reduction}
	Recall system \eqref{EQVECEQ2}:
	\[
		\systeme[tsr]{
			t+2s-2r=-15@\qquad\text{row}_1,
			2t+s-5r=-21@\qquad\text{row}_2,
			t-4s+r=18@\qquad\text{row}_3
		}
	\]
	When performing elimination, there was a lot of redundant information. 
	In particular, the variables and the ``$=$'' never changed---it was
	only the numbers that changed. To make our lives easier, we will
	write a this system as an \emph{augmented matrix}\index{augmented matrix}\footnote{
	A \emph{matrix} is just a box of numbers. An \emph{augmented matrix} is a matrix
	with an extra column.
	}.
	\[
		\systeme[tsr]{
			t+2s-2r=-15,
			2t+s-5r=-21,
			t-4s+r=18
		}\qquad\text{corresponds to}\qquad
		\begin{bmatrix}[rrr|r]
			1&2&-2 & -15\\
			2&1&-5&-21\\
			1&-4&1&18
		\end{bmatrix}
	\]
	We use a vertical line in our matrix to separate coefficients of our unknowns
	from numbers on the right side of the ``$=$''. Written with matrices, elimination
	becomes easier to perform by hand.
	\begin{align*}
		\begin{bmatrix}[rrr|r]
			1&2&-2 & -15\\
			2&1&-5&-21\\
			1&-4&1&18
		\end{bmatrix}
		&\xrightarrow{\text{row}_3\mapsto\text{row}_3-\text{row}_1}
		\begin{bmatrix}[rrr|r]
			1&2&-2 & -15\\
			2&1&-5&-21\\
			0&-6&3&33
		\end{bmatrix}\\
		&\xrightarrow{\text{row}_2\mapsto\text{row}_2-2\text{row}_1}
		\begin{bmatrix}[rrr|r]
			1&2&-2 & -15\\
			0&-3&-1&9\\
			0&-6&3&33
		\end{bmatrix}\\
		&\xrightarrow{\text{row}_3\mapsto\text{row}_3-2\text{row}_3}
		\begin{bmatrix}[rrr|r]
			1&2&-2 & -15\\
			0&-3&-1&9\\
			0&0&5&15
		\end{bmatrix}
	\end{align*}
From here, we can solve the original system by substitution---the last
row of the matrix corresponds to the equation $5r=15$, just as before.
Notice that if we multiplied the last row of the augmented matrix by $\tfrac{1}{5}$,
we would be left with the equation $r=3$ and could continue eliminating.
Since working with augmented matrices is so fun, let's keep eliminating!
	\begin{align*}
		\begin{bmatrix}[rrr|r]
			1&2&-2 & -15\\
			0&-3&-1&9\\
			0&0&5&15
		\end{bmatrix}
		&\xrightarrow{\text{row}_3\mapsto\tfrac{1}{5}\text{row}_3}
		\begin{bmatrix}[rrr|r]
			1&2&-2 & -15\\
			0&-3&-1&9\\
			0&0&1&3
		\end{bmatrix}
		&\xrightarrow{\text{row}_2\mapsto\text{row}_2+\text{row}_3}
		\begin{bmatrix}[rrr|r]
			1&2&-2 & -15\\
			0&-3&0&12\\
			0&0&1&3
		\end{bmatrix}\\
		&\xrightarrow{\text{row}_1\mapsto\text{row}_1+2\text{row}_3}
		\begin{bmatrix}[rrr|r]
			1&2&0 & -9\\
			0&-3&0&12\\
			0&0&1&3
		\end{bmatrix}
		&\xrightarrow{\text{row}_2\mapsto\tfrac{-1}{3}\text{row}_2}
		\begin{bmatrix}[rrr|r]
			1&2&0 & -9\\
			0&1&0&-4\\
			0&0&1&3
		\end{bmatrix}\\
		&\xrightarrow{\text{row}_1\mapsto\text{row}_1-2\text{row}_2}
		\begin{bmatrix}[rrr|r]
			1&0&0 & -1\\
			0&1&0&-4\\
			0&0&1&3
		\end{bmatrix}\\
	\end{align*}
Now we have something truly special. If we turn the augmented
matrix back into a system of equations, we have that 
	\[
		\begin{bmatrix}[rrr|r]
			1&0&0 & -1\\
			0&1&0&-4\\
			0&0&1&3
		\end{bmatrix}\\
		\qquad\text{corresponds to}\qquad
		\systeme[tsr]{
			t\phantom{++}=-1,
			s\phantom{++}=-4,
			r=3
		}
	\]
The system's solution can be read right from the matrix!

The process of writing a system as an augmented matrix and manipulating
the rows of the matrix until a simpler system is obtained is called
\emph{row reduction}.

\subsection{The Row-reduction Algorithm}
The beauty of row reduction is that it can be done to any matrix. Not only
that, but there is an algorithm, called \emph{Gauss's algorithm}\footnote{ Gauss
popularized this algorithm in the West, but it was known to the ancient Chinese
long before Gauss's time.}, that allows you to perform row reduction without
the need for any creativity.

First, we present the possible operations used in row reduction.
\begin{definition}[Elementary Row Operations]
	The \emph{elementary row operations}\index{elementary row operation}
	are the three operations of
	\begin{enumerate}
		\item swapping two rows (written $\text{row}_i\leftrightarrow\text{row}_j$);
		\item multiplying a row by a non-zero scalar (written
		$\text{row}_i\mapsto \alpha\,\text{row}_i$); and
		\item adding a multiple of one row to a different row (written $\text{row}_i\mapsto
		\text{row}_i+\alpha\,\text{row}_j$).
	\end{enumerate}
\end{definition}
\begin{theorem}
	Every elementary row operation has an inverse which is also an elementary
	row operation. In other words, every elementary row operation can be undone.
\end{theorem}
\begin{proof}
	We can explicitly write down the inverses of each elementary row operation.
	$\text{row}_i\leftrightarrow\text{row}_j$ is its own inverse. Since
	$\alpha\neq 0$, we have that $\text{row}_i\mapsto \alpha\,\text{row}_i$
	and $\text{row}_i\mapsto \tfrac{1}{\alpha}\,\text{row}_i$ are inverses.
	Finally, $\text{row}_i\mapsto
		\text{row}_i+\alpha\,\text{row}_j$
		and $\text{row}_i\mapsto
		\text{row}_i-\alpha\,\text{row}_j$ are inverses.
\end{proof}

\begin{definition}[Equivalent Systems]
	Two systems of equations, $(X)$ and $(Y)$, are called \emph{equivalent},
	written $(X)\sim(Y)$ if they have
	exactly the same solution(s). 
	
	Two matrices are called equivalent if their corresponding systems are equivalent.
\end{definition}

\begin{theorem}
	Let $(X)$ be a system of equation and let $(Y)$ be
	the result of applying any number of elementary
	row operations to $(X)$.  Then, $(X)$ and $(Y)$ are equivalent systems.
\end{theorem}
\begin{proof}
	Let $(X)$ be a system of equations and let $R(X)$ be the result of applying
	a single row operation to $(X)$.
	Because equivalence of systems is transitive\footnote{ That
	is, if $(X)\sim(Y)$ and $(Y)\sim(Z)$, then $(X)\sim (Z)$}, it is sufficient
	to show $(X)\sim R(X)$.

	Note that every row operation obeys the \emph{law of algebraic manipulation}.
	That is, a row operation takes a list of true statements (the equations) to another 
	list of true statements (an new list of equations). As such, if the statement
	\[
		\vec x\text{ is a solution to }(X)
	\]
	is true, then the statement 
	\[
		\vec x\text{ is a solution to }R(X)
	\]
	must also be true.  Phrased compactly, the law of algebraic manipulation
	ensures that
	\[
		\vec x\text{ is a solution to }(X)\qquad \text{implies}
		\qquad \vec x\text{ is a solution to }R(X).
	\]

	To show $(X)\sim R(X)$, we need to prove that
	\[
		\vec x\text{ is a solution to }R(X)\qquad \text{implies}
		\qquad \vec x\text{ is a solution to }(X).
	\]
	
	Fix a row operation $R$ and let $R^{-1}$ denote its inverse. Since
	$R^{-1}$ is also an elementary row operation, by the law of
	algebraic manipulation, we have that
	\[
		\vec x\text{ is a solution to }(X)\qquad \text{implies}
		\qquad \vec x\text{ is a solution to }R^{-1}(X)
	\]
	and so
	\[
		\vec x\text{ is a solution to }R(X)\qquad \text{implies}
		\qquad \vec x\text{ is a solution to }R\circ R^{-1}(X) = (X).
	\]
	Thus, $(X)\sim R(X)$.
\end{proof}

Now that we have established that applying elementary row operations produces
equivalent systems, we are ready to give a row-reduction algorithm.

\begin{definition}[Row Reduction Algorithm]
	Let $M$ be a matrix.
	\begin{enumerate}
		\item If $M$ takes the form $M=[\vec 0|M']$ (that is, its first column
		is all zeros), apply the algorithm to $M'$;
		\item if not, perform a row-swap so the upper-left entry of $M$ is
			non-zero.
		\item Perform the row operation $\text{row}_1\mapsto \tfrac{1}{\alpha}\text{row}_1$
		where $\alpha$ is the upper-left entry of $M$. This entry is referred to as
		a \emph{pivot}.
		\item Use the row operation $\text{row}_i\mapsto \text{row}_i-\beta\text{row}_1$
		to zero every entry below the pivot.
		\item Now $M$ has the form
		\[
			M=\begin{bmatrix}[c|c]
				1 & ??\\
				\hline\\[\dimexpr-\normalbaselineskip+2pt]
				\vec 0 & M'
			\end{bmatrix}.
		\]
		Apply the algorithm to $M'$.

	\end{enumerate}

	The resulting matrix is now in \emph{row echelon form}. To put the matrix in 
	\emph{reduced row echelon form}, additionally apply step 6.
	\begin{enumerate}
		\item[6.] Use the row operation $\text{row}_i\mapsto \text{row}_i+\alpha\text{row}_j$
		to zero above each pivot.
	\end{enumerate}
\end{definition}

Stated all at once, this algorithm can be hard to follow, but with practice it becomes
straightforward.

\begin{example}
	Apply the row-reduction algorithm to the matrix
	\[
		M=\mat{0&0&0&-2&-2\\0&1&2&3&2\\0&2&4&5&3}.
	\]
	
	First notice that $M$ starts with a column of zeros, so we will focus on
	the right side of $M$. We will draw a line to separate it.
	\[
	M=\begin{bmatrix}[r|rrrr]
		0&0&0&-2&-2\\0&1&2&3&2\\0&2&4&5&3
	\end{bmatrix}
	\]
	Next, we perform a row swap to bring a non-zero entry to the upper left.
	\[
	\begin{bmatrix}[r|rrrr]
		0&0&0&-2&-2\\0&1&2&3&2\\0&2&4&5&3
	\end{bmatrix}
	\xrightarrow{\text{row}_1\leftrightarrow\text{row}_2}
	\begin{bmatrix}[r|rrrr]
		0&1&2&3&2\\0&0&0&-2&-2\\0&2&4&5&3
	\end{bmatrix}
	\]
	The upper-left entry is already a $1$, so we can use it to zero all entries below.
	\[
	\begin{bmatrix}[r|rrrr]
		0&1&2&3&2\\0&0&0&-2&-2\\0&2&4&5&3
	\end{bmatrix}
	\xrightarrow{\text{row}_3\mapsto\text{row}_3-2\text{row}_1}
	\begin{bmatrix}[r|rrrr]
		0&1&2&3&2\\0&0&0&-2&-2\\0&0&0&-1&-1
	\end{bmatrix}
	\]
	Now we work on the submatrix.
	\[
	\begin{bmatrix}[rr|rrr]
		0&1&2&3&2\\
		\hline\\[\dimexpr-\normalbaselineskip+2pt]
		0&0&0&-2&-2\\0&0&0&-1&-1
	\end{bmatrix}
	\]
	Again, the submatrix has a first column of zeros, so we pass to a sub-submatrix.
	\[
	\begin{bmatrix}[rrr|rr]
		0&1&2&3&2\\
		\hline\\[\dimexpr-\normalbaselineskip+2pt]
		0&0&0&-2&-2\\0&0&0&-1&-1
	\end{bmatrix}
	\]
	Now we turn the upper left entry into a $1$ and use that pivot
	to zero all entries below.
	\[
	\begin{bmatrix}[rrr|rr]
		0&1&2&3&2\\
		\hline\\[\dimexpr-\normalbaselineskip+2pt]
		0&0&0&-2&-2\\0&0&0&-1&-1
	\end{bmatrix}
	\xrightarrow{\text{row}_2\mapsto\tfrac{-1}{2}\text{row}_2}
	\begin{bmatrix}[rrr|rr]
		0&1&2&3&2\\
		\hline\\[\dimexpr-\normalbaselineskip+2pt]
		0&0&0&1&1\\0&0&0&-1&-1
	\end{bmatrix}
	\xrightarrow{\text{row}_3\mapsto\text{row}_3+\text{row}_2}
	\begin{bmatrix}[rrr|rr]
		0&1&2&3&2\\
		\hline\\[\dimexpr-\normalbaselineskip+2pt]
		0&0&0&1&1\\0&0&0&0&0
	\end{bmatrix}
	\]
	The matrix is now in row echelon form. To put it in reduced row echelon
	form, we zero above each pivot.
	\[
	\begin{bmatrix}
		0&1&2&3&2\\
		0&0&0&1&1\\0&0&0&0&0
	\end{bmatrix}
	\xrightarrow{\text{row}_1\mapsto\text{row}_1-3\text{row}_2}
	\begin{bmatrix}
		0&1&2&0&-1\\
		0&0&0&1&1\\0&0&0&0&0
	\end{bmatrix}
	\]
\end{example}

The row-reduction algorithm applies to any matrix, whether augmented or not. However,
whether or not the reduced form of a matrix has any \emph{meaning} depends on where
the matrix came from in the first place.

	
\subsection{Geometry of Systems of Linear Equations}
We encountered systems of linear equations when trying to answer questions about
span and linear independence, but they are worth thinking about in their own right.
Consider the system
\begin{equation}
\label{EQSYSGEO}
		\systeme{
			x+2y-2z=-15,
			2x+y-5z=-21,
			x-4y+z=18
		}
\end{equation}
A solution to this system is a tuple $(x,y,z)$ that satisfies every equation simultaneously.
Taken individually, each equation in this system represents a plane. Let
\begin{align*}
	\mathcal P_1&=\Set*{(x,y,z)\in\R^3\given x+2y-2z=-15}\\
	\mathcal P_2&=\Set*{(x,y,z)\in\R^3\given 2x+y-5z=-21}\\
	\mathcal P_3&=\Set*{(x,y,z)\in\R^3\given x-4y+z=18}
\end{align*}
be the planes corresponding to the equations in system \eqref{EQSYSGEO}.
We can think of solutions to system \eqref{EQSYSGEO} as points in $\mathcal P_1\cap 
\mathcal P_2\cap \mathcal P_3$.

Geometrically, we now have an immediate intuition for the what the solution
sets to systems with three equations and three unknowns look like. Three planes
can either intersect in a plane (if they are all the same plane), a line, a point,
or they may not have a common intersection.

XXX Figure

If a system of equations has only two unknowns, then the solution set must look
like the intersection of lines. Namely, the solution set is a line, a point, or
empty.

XXX Figure

This logic generalizes to higher dimensions: the set of solutions to a system of linear
equations is always a ``flat'' object (a line, plane, volume, etc.), a point, or empty.
This is captured in the following theorem, which we will prove later.
\begin{theorem}
	Let $(X)$ be a system of linear equations. Then $(X)$ either has
	no solutions, one solution, or infinitely many solutions.
\end{theorem}

\subsection{Free Variables}
By now we are very familiar with the system
\begin{equation}
		\systeme{
			x+2y-2z=-15,
			2x+y-5z=-21,
			x-4y+z=18
		}
\end{equation}
which has a solution $(x,y,z)=(-1,-4,3)$. When we use the row reduction algorithm
on an augmented matrix, we get
\[
		\begin{bmatrix}[rrr|r]
			1&2&-2 & -15\\
			2&1&-5&-21\\
			1&-4&1&18
		\end{bmatrix}
		\qquad\sim\qquad
		\begin{bmatrix}[rrr|r]
			1&0&0 & -1\\
			0&1&0&-4\\
			0&0&1&3
		\end{bmatrix},
\]
and we can read the unique solution directly from the matrix. But what happens when there isn't a 
unique solution?

Consider the system
\begin{equation}
	\label{EQFREEVAR}
	\systeme{x+3y=2,2x+6y=4}.
\end{equation}
When using an augmented matrix to solve this system, we run into an issue.
\[
		\begin{bmatrix}[rr|r]
			1&3 & 2\\
			2&6&4\\
		\end{bmatrix}
		\qquad\sim\qquad
		\begin{bmatrix}[rr|r]
			1&3&2\\
			0&0&0\\
		\end{bmatrix}
\]
From the reduced row echelon form we're left with the equation $x+3y=2$, which isn't exactly
a \emph{solution}. Effectively, the original system had only one equation's worth of information,
so we cannot solve for both $x$ and $y$ based on the original system. To get ourselves out of
this pickle, we will use a notational trick: introduce the arbitrary equation $y=t$.
Now, because we've already done row-reduction, we see
\[
	\systeme{x+3y=2,2x+6y=4,y=t}\qquad\sim\qquad
	\systeme{x+3y=2,y=t}.
\]
Here we've omitted the equation $0=0$ since it adds no information.
We can write the solution to this system as
\[
	\vec x=\mat{x\\y} = \matc{2-3t\\t}=t\mat{-3\\1}+\mat{2\\0}.
\]
Notice that $t$ here stands for an arbitrary real number. And choice of $t$
produces a valid solution to the original system (go ahead, pick some values
for $t$ and see what happens).  We call $t$ a \emph{parameter} and $y$ a
\emph{free variable}\footnote{ We call $y$ \emph{free} because we may pick
it to be anything we want and still produce a solution to the system.}.
Notice further that 
\[
	\vec x=t\mat{-3\\1}+\mat{2\\0}
\]
is vector form of the line $x+3y=2$.

If a system of equations has infinitely many solutions, solving it will
require picking a free variable. You have a lot of choice over which variables
you pick to be free, but there is an algorithmic way to pick free variables that leaves
no room for failure.
\begin{definition}[Canonical Choice of Free Variables]
	Let $(X)$ be a system of linear equations in the variables
	$x_1,\ldots, x_n$, and let $R$ be the corresponding
	row-reduced matrix. The variable $x_i$ is a \emph{canonical
	free variable} if the $i$th column of $R$ does not contain a pivot.
\end{definition}
By picking assigning parameters to the canonical free variables, you are guaranteed
to be able to write down all solutions to a system of linear equations.

\begin{example}
	XXX Finish
\end{example}

XXX Finish. Complete solutions, etc.?

\subsection{Consistent and Inconsistent Systems}

XXX Finish


\section{Subspaces \& Bases}

	Lines or planes through the origin can be written as spans
	of their direction vectors. However, a line or plane that doesn't
	pass through the origin cannot be written as a span---it must
	be expressed as a \emph{translated} span.

	XXX Figure


	There's something special about sets that can be expressed as 
	(untranslated) spans. In particular, since a linear combination
	of linear combinations is still a linear combination, a span
	is \emph{closed} with respect to linear combinations. That is, 
	by taking linear combinations of vectors in a span, you cannot
	escape the span. In general, sets that have this property are called
	\emph{subspaces}\index{subspace}.

	\begin{definition}[Subspace]
		A subset $\mathcal V\subseteq \R^n$ is called a \emph{subspace}
		if the following two properties are satisfied.
		\begin{enumerate}
			\item[(i)] $\vec v\in\mathcal V$ implies $\alpha\vec v\in\mathcal V$
			for all $\alpha$.
			\item[(ii)] $\vec u,\vec v\in \mathcal V$ implies $\vec u+\vec v\in\mathcal V$.
		\end{enumerate}
	\end{definition}

	The properties listed in the definition of a subspace are a succinct
	way of stating that a linear combinations of vectors in a subspace
	remain in that subspace\footnote{ An even more succinct
	way of defining a subspace $\mathcal V$ is that if $\vec u,\vec v\in\mathcal V$,
	then $\alpha\vec u+\vec v\in\mathcal V$ for all scalars $\alpha$.}.
	Subspaces generalize the idea of \emph{flat spaces through the origin}. They encompass
	lines, planes, volumes and more.

	\begin{example}
		Let $\mathcal V\subseteq \R^2$ be the complete solution to 
		$x+2y=0$.  Show that $\mathcal V$ is a subspace.

		XXX Finish
	\end{example}

	\begin{example}
		Let $\mathcal W\subseteq \R^2$ be the line expressed in vector form
		as 
		\[
			\vec x=t\mat{1\\2}+\mat{1\\1}.
		\]
		Determine whether $\mathcal W$ is a subspace.

		XXX Finish
	\end{example}


	As mentioned earlier, subspaces and spans are deeply connected
	by the following theorem.

	\begin{theorem}
		Every subspace is a span and every span is a subspace.  More precisely,
		$\mathcal V\subseteq \R^n$ is a subspace if and only if $\mathcal V=
		\Span\mathcal X$ for some set $\mathcal X$.
	\end{theorem}
	\begin{proof}
		We will start by showing every span is a subspace.  Fix $\mathcal X\subseteq\R^2$
		and let
		$\mathcal V=\Span\mathcal X$. By definition, $\vec v\in\mathcal V$ means that
		\[
			\vec v=\sum \alpha_i\vec x_i
		\]
		for some $\vec x_i\in\mathcal X$ and scalars $\alpha_i$. It follows that
		\[
			\alpha\vec v=\alpha\sum \alpha_i\vec x_i = \sum (\alpha\alpha_i)\vec x_i\in
			\Span\mathcal X=\mathcal V.
		\]
		Similarly, if $\vec u\in\mathcal V$, then
		\[
			\vec u=\sum\beta_i\vec y_i
		\]
		for some $\vec y_i\in\mathcal X$ and scalars $\beta_i$. It follows that
		\[
			\vec u+\vec v=\sum\alpha_i\vec x_i + \sum\beta_i\vec y_i
		\]
		is also a linear combination of vectors in $\mathcal X$, and so $\vec u+\vec v\in
		\Span\mathcal X=\mathcal V$. Thus, $\mathcal V$ is a subspace.

		Now we will prove that every subspace is a span. Let $\mathcal V$ be a subspace
		and consider $\mathcal V'=\Span\mathcal V$. We know that 
		$\mathcal V\subseteq \mathcal V'$. If we establish that $\mathcal V'\subseteq\mathcal V$,
		then $\mathcal V=\mathcal V'=\Span\mathcal V$, which would complete the proof.

		Fix $\vec x\in\mathcal V'$. By definition, 
		\[
			\vec x=\sum \alpha_i\vec v_i
		\]
		for some $\vec v_i\in\mathcal V$ and scalars $\alpha_i$. Observe that 
		$\alpha_i\vec v_i\in\mathcal V$ since $\mathcal V$ is closed under scalar
		multiplication. Thus, $\alpha_1\vec v_1+\alpha_2\vec v_2\in\mathcal V$
		because $\mathcal V$ is closed under sums. Continuing, 
		$(\alpha_1\vec v_1+\alpha_2\vec v_2)+\alpha_3\vec v_3\in\mathcal V$ because
		$\mathcal V$ is closed under sums. It follows that
		\[
			\vec x=\sum \alpha_i\vec v_i
			=\Big(\big((\alpha_1\vec v_1+\alpha_2\vec v_2)
			+\alpha_3\vec v_3\big)+\cdots+\alpha_{n-1}\vec v_{n-1} \Big)
			+\alpha_n\vec v_n\in\mathcal V.
		\]
		Thus $\mathcal V'\subseteq\mathcal V$, which completes the proof.
	\end{proof}


\subsection{Basis}
	Let $\vec d=\mat{1\\2}$ and consider $\ell=\Span\Set{\vec d}$.

	XXX Figure

	We know that since $\ell$ is a subspace, $\ell=\Span\ell$. However,
	the simplest descriptions of $\ell$ involve the span of only one vector.


	Analogously, let $\mathcal P=\Span\Set{\vec d_1,\vec d_2}$ be the plane
	through the origin with direction vectors $\vec d_1$ and $\vec d_2$. There
	are many ways to write $\mathcal P$ as a span, but the simplest ones
	involve exactly two vectors. The simplest way to describe a subspace
	as a span is captured in the idea of \emph{basis}\index{basis}.

	\begin{definition}[Basis]
		Let $\mathcal V$ be a subspace. A \emph{basis} for 
		$\mathcal V$ is a linearly independent set $\mathcal B$
		such that $\mathcal V=\Span\mathcal B$.
	\end{definition}

	In short, a basis for a subspace is a linearly independent set that spans that
	subspace.

	\begin{example}
		\label{EXLINEBASIS}
		Let $\ell=\Span\Set*{\mat{1\\2},\mat{-2\\-4}, \mat{1/2\\1}}$. Find
		a basis for $\ell$.

		XXX Finish
	\end{example}

	Bases for subspaces are not unique. For a line through the origin, 
	any non-zero direction vector serves as a basis. However, every basis
	must have the same size.

	\begin{theorem}
		Let $\mathcal X$ and $\mathcal Y$ both be bases
		for the subspace $\mathcal V$. Then $\Abs{\mathcal X}=\Abs{\mathcal Y}$.
		That is, $\mathcal X$ and $\mathcal Y$ have the same size.
	\end{theorem}
	\begin{proof}
		Let $\mathcal X=\Set{\vec x_1,\ldots, \vec x_m}$ and $\mathcal Y=\Set{\vec y_1,
		\ldots,\vec y_n}$ both be bases for $\mathcal V$.
		Without loss of generality, assume $\Abs{\mathcal X}>\Abs{\mathcal Y}$.
		We will replace the elements of $\mathcal X$ with elements of $\mathcal Y$
		one by one.

		Consider the set $\mathcal X\cup\Set{\vec y_1}$. Since
		$\vec y_1\in\mathcal V=\Span\mathcal X$, the set
		$\mathcal X\cup\Set{\vec y_1}$ must be linearly dependent.
		Further, since $\mathcal Y$ is a basis, $\Set{\vec y_1}$ is a linearly
		independent set. We conclude that there must exist some $0\leq {r_1}\leq m$
		so that
		\[
			\vec x_{r_1}\in\Span\, (\mathcal X\cup\Set{y_1})\backslash \Set{x_{r_1}}.
		\]
		Let $\mathcal B_1=(X\cup\Set{\vec y_1})\backslash \Set{\vec x_{r_1}}$ and note that
		$\Abs{\mathcal B_1}=\Abs{\mathcal X}$ and that $\mathcal V=\Span\mathcal B_1$.

		Continuing, consider $\mathcal B_1\cup\Set{\vec y_2}$. Since $\Set{\vec y_1,\vec y_2}$
		is a linearly independent set, there must exist some $0\leq r_2\leq m$ with $r_2\neq r_1$
		so that
		\[
			\vec x_{r_2}\in\Span\, (\mathcal B_1\cup\Set{y_2})\backslash \Set{\vec x_{r_2}}.
		\]
		Let $\mathcal B_2=(\mathcal B_1\cup\Set{\vec y_2})\backslash \Set{\vec x_{r_2}}$.

		Continuing in this way, construct the sets $\mathcal B_3,\ldots,\mathcal B_n$ where
		\[
			\mathcal B_n=\mathcal Y\cup (\mathcal X\backslash \Set{\vec x_{r_1},\ldots,\vec x_{r_n}}).
		\]
		Since $\Abs{\mathcal X}>\Abs{\mathcal Y}$, we know $\mathcal Y\subsetneq \mathcal B_n$.
		Since $\mathcal Y$ is a basis, we conclude that $\mathcal B_n$ must be 
		linearly dependent. However, any dependency relationship in $\mathcal B_n$ corresponds
		to a dependency relationship in $\mathcal X$, which is a contradiction.
	\end{proof}

	Since all bases for a particular subspace have the same size, we can
	refer to \emph{the} size of \emph{a} basis for a subspace---this number is special
	and is called the \emph{dimension}\index{dimension} of the subspace.

	\begin{definition}[Dimension]
		Let $\mathcal V$ be a subspace. The \emph{dimension} of $\mathcal V$
		is the size of a basis for $\mathcal V$.
	\end{definition}

	Using this definition, the dimension of a plane is $2$, of a line is $1$,
	and of a point is $0$\footnote{ The dimension of a line, plane, or point
	not through the origin is defined to be the dimension of the subspace obtained
	when translating it to the origin.}.

	\begin{example}
		Find the dimension of $\R^2$.

		Since $\Set{\xhat, \yhat}$ is a basis for $\R^2$, we know $\R^2$ is
		two dimensional.
	\end{example}

	\begin{example}
		Let $\ell=\Span\Set*{\mat{1\\2},\mat{-2\\-4}, \mat{1/2\\1}}$.

		In Example \ref{EXLINEBASIS}, we found XXX was a basis for $\ell$.

		XXX Finish
	\end{example}

	\begin{definition}[Standard Basis for $\R^n$]
		The \emph{standard basis} for $\R^n$ are the 
		vectors $\vec e_1,\ldots,\vec e_n$ where
		\[
			\vec e_1=\matc{1\\0\\0\\\vdots}\qquad
			\vec e_2=\matc{0\\1\\0\\\vdots}\qquad
			\vec e_3=\matc{0\\0\\1\\\vdots}\qquad\cdots.
		\]
		That is $\vec e_i$ is the vector with a $1$ in its
		$i$th component and zeros elsewhere.
	\end{definition}

	The notation $\vec e_i$ is context specific. If we say $\vec e_i\in\R^2$,
	then $\vec e_i$ must have exactly two components. If we say $\vec e_i\in\R^{45}$,
	then $\vec e_i$ must have $45$ components. 

\section{Matrix Equations}

\subsection{Images}

There is one more important way to create geometric objects, and that is using functions
and the idea of \emph{image}\index{image}.

\DefImage

In plain language, the image of a set $X$ under a function $f$ is the set
of all outputs of $f$ when only points from $X$ are input.

\begin{example}
	Let $f:\R^2\to\R^2$ be defined as $f(x,y)=(2x,y)$ and let $C=\Set{\vec x\in\R^2\given
	\norm{\vec x}=1}$ be the unit circle. Find $f(C)$.

	The image $f(C)$ consists of all the points of $C$ after $f$ is applied.
	Since $f$ doubles the $x$-coordinate, we $f(C)$ will be $C$ stretched in the $x$-direction
	by $2$.

	XXX Figure
\end{example}

Images allow exotic transformations of sets. For example, consider \[f(x,y)=((x+y)\sin x, (x+y)\cos x),\]
and let $X=\Set{t\xhat\given t\geq 0}$ be the positive $x$-axis. The image $f(X)$ is a spiral!

XXX Figure

Nearly any geometric figure can be made using images and cleverly chosen functions, and multi-variable
calculus studies how arbitrary functions (via taking images) change geometry. For our purposes, we will
stick mainly with down to earth functions: linear functions\footnote{ Linear functions
have the property that they always take lines to lines.} and translations. But, we can still get a 
lot of utility out of images even in the restricted case of linear functions and translations.

Let $T=\Set{\vec x\in\R^2\given \vec x=\alpha\xhat+\beta\yhat\text{ where } \alpha,\beta\geq 0\text{ and }
\alpha+\beta \leq 1}$ be a triangle and let $R:\R^2\to\R^2$ be the function that rotates vectors 
counter-clockwise by 90$^\circ$. That is $R(x,y)=(-y,x)$. And, let $V:\R^2\to\R^2$ defined by $V(\vec a)=
\vec a+\yhat$ be vertical translation. Then, we can start making some fun pictures!

XXX Figure ({T, R(T)}, {T, V of R(T)}, ...)


