
Multivariable calculus has a long history with many refinements over the years.
Every once in a while, a mathematician will notice a connection
that wasn't explicit before.  You may have noticed that many of the
definitions we used were not particular to any specific dimension, but the problems
we solved were usually in $\R^2$ and $\R^3$.  Sometimes we relied
on the cross product, which requires us to
work in $\R^3$.  
Subjects like \emph{General Relativity} study
calculus in $\R^4$ (or higher dimensions), but how, for example,
would we compute a surface integral in $\R^4$?  We would need
to find a volume form/surface element without the use of the cross product in a dimension
where our geometric intuition is diminished, to say the least.

\emph{Differential forms}\index{differential forms} are one solution to this issue.
They recast many of the fundamental ideas---gradients, curl, divergence,
line integrals, surface integrals, etc.---in a unified framework that is consistent
regardless of dimension and \emph{regardless of coordinate system}.
Because of their abstract foundation and newness\footnote{ 
The foundation for differential forms was laid out Hermann Grassmann
in 1847, and it wasn't until Albert Einstein needed a new kind
of mathematics to describe relativity that differential forms
started gaining in popularity.}, differential forms they are not typically encountered in a
first course on multivariable calculus, but that has more to do with culture
than anything else.

\section{Abstract Foundation for Differential Forms}

Differential forms have close ties with calculus and linear algebra.
For that reason, we will use some terms from linear algebra, including the
abstract idea of a \emph{vector}\index{vector space}.

\begin{definition}[Vector Space]
	A set $V$, coupled with an addition and a
	scalar multiplication is called a \emph{vector space} if
	the following properties hold for all $\vec u,\vec v,\vec w\in V$
	and all scalars $\alpha$ and $\beta$:
	\begin{align*}
		\vec u+\vec v\in V\quad&\text{and}\quad \alpha\vec u\in V\tag{Closure}\\
		(\vec u+\vec v)+\vec w&=\vec u+(\vec v+\vec w)\tag{Associativity}\\
		\vec u + \vec v - \vec v &= \vec u+\vec 0 = \vec u\tag{Identity}\\
		\vec u+\vec v&=\vec v+\vec u\tag{Commutativity}\\
		\alpha(\vec u+\vec v)&=\alpha\vec u+\alpha \vec v\tag{Distributivity}\\
		(\alpha\beta)\vec v&=\alpha(\beta \vec v)\tag{Scalar Associativity}\\
		(\alpha+\beta)\vec v&=\alpha\vec v+\beta \vec v\tag{Scalar Distributivity}
	\end{align*}
\end{definition}

These properties should look familiar from Section \ref{SECVECTORPROPS}, and indeed
$\R^n$ is a vector space.  However there are many seemingly non-geometric
sets that are also vector spaces.

\begin{example}
    \label{EXVECSPACEEXAMPLE}
	Let $V=\Set{\text{functions from }\R\text{ to }\R}$ equipped with
	pointwise addition and scalar multiplication.  Then, $V$ is a vector space.

	We won't prove every property in detail, but if we pick $f,g\in V$,
	we need the quantities $f+g$ and $\alpha f$ to make sense and to also
	be in $V$.  This becomes almost trivially true, thanks to the definition
	of addition of functions.

	Let $h=f+g$.  By definition $h(x) = f(x)+g(x)$, and so $h$ is
	a function from $\R$ to $\R$.  Therefore $h\in V$.  Further
	$q=\alpha h$ is defined by $q(x) = \alpha h(x)$, and so $q\in V$.
	Thus, $V$ satisfies the closure property.  The rest of the properties
	follow via similar arguments.
\end{example}
Recall that if $\vec v_1,\ldots, \vec v_n\in V$ are vectors, then 
$\vec w=\alpha_1\vec v_1+\cdots +\alpha_n\vec v_n$ is a 
\emph{linear combination}\index{linear combination} of the vectors $\vec v_1,\ldots \vec v_n$.
Another way to think of a vector space is as a set on which linear combinations
make sense.

\bigskip
Now that we have abstract ``vectors'' (elements of a vector space), let's
get abstract lines and planes.  These objects can be defined using
\emph{linear functions}\index{linear transformation} which are equivalently
called \emph{linear transformations}\footnote{ The words \emph{function},
\emph{transformation}, and \emph{mapping} are synonymous in math.  We like
to mix it up some times.}.

\begin{definition}[Linear]
	Let $V$ and $W$ be vector spaces and let
	$T:V\to W$ be a function.  $T$ is called \emph{linear}
	if for all $\vec u,\vec v\in V$ and all scalars $\alpha$,
	\[
		T(\vec u+\vec v) = T(\vec u)+T(\vec v)\qquad\text{and}\qquad
		T(\alpha \vec v) = \alpha T(\vec v).
	\]
\end{definition}
If a transformation is linear, it is typical to omit parentheses when
passing in a single vector.  That is, instead of writing $T(\vec v)$,
we write $T\vec v$, but we would always write $T(\vec u+\vec v)$
with parentheses.

We have already encountered many linear functions.  For example, 
consider $f:\R^3\to\R$ where
$f(\vec v) = \xhat\cdot \vec v$.  Based on the properties of the dot product,
\[
	f(\vec u+\vec v) = \xhat\cdot (\vec u+\vec v)=
	\xhat\cdot \vec u+\xhat \cdot \vec v = f(\vec u)+f(\vec v)
\]
and 
\[
	f(\alpha \vec v) = \xhat \cdot (\alpha \vec v) = \alpha \xhat \cdot \vec v
	=\alpha f(\vec v).
\]

Another linear transformation that we have encountered is projection\index{projection}.
For a fixed vector $\vec u$, let $f:\R^3\to\R^3$ be defined by 
$f(\vec v) = \Proj_{\vec u} \vec v$.

\begin{exercise}
	Show that for a fixed $\vec u$, the function 
	$f(\vec v) = \Proj_{\vec u} \vec v$ is linear.
\end{exercise}

Linear functions help us define lines, planes, etc. via their \emph{kernel}\index{kernel}.

\begin{definition}[Kernel]
	For a function $T:V\to W$, the \emph{kernel} (or \emph{null space}
	if $T$ is a linear function)
	of $T$ is defined to be 
	\[
		\Ker(T) = \Set{\vec v\in V\given T(\vec v) = \vec 0}.
	\]
\end{definition}

Thinking at this level of abstraction can be challenging at first,
but we've really been working with these ideas for a long time.

For example, let $\vec n = (1,2,3)$ and let $P:\R^3\to\R$ be defined by
$P(\vec v) = \vec n\cdot \vec v$.  We've already seen that $P$ is linear.
What is $\Ker(P)$?  Well,
\[
	P\mat{x\\y\\z} = \mat{1\\2\\3}\cdot \mat{x\\y\\z} = 
	x+2y+3z=0
\]
exactly when $(x,y,z)$ lies on the plane through the origin with normal vector $\vec n$.

Lines can be written as kernels as well.

\begin{exercise}
	Let $\vec d=(1,1,1)$.  Find a linear transformation $L$ so that
	$\Ker(L)$ is the line through $\vec 0$ with direction vector $\vec d$.
	(It may help to think of a line as the intersection of two planes.)
\end{exercise}

Unfortunately, linear functions cannot describe \emph{all} ``flat''
objects.  The kernel of a linear function always contains $\vec 0$, and
so lines and planes that do not pass through the origin cannot be described
as kernels of linear functions.  However, they can be described as kernels
of the related \emph{affine functions}\index{affine function}.

\begin{definition}[Affine Function]
	Let $V$ and $W$ be vector spaces.  A function $A:V\to W$ is called
	\emph{affine} if there exists some $\vec w\in W$ and some linear transformation
	$T:V\to W$ so that
	\[
		A(\vec v) = T(\vec v) + \vec w
	\]
	for all $\vec v$.
\end{definition}

You may recall previously, in Section \ref{SECAFFINE},
we only defined what it means for a function $f:\R^n\to \R$
to be affine.
We now have a fully general
definition!

\begin{exercise}
	Write the plane with normal vector $\vec n=(1,2,3)$ and which
	passes through the point $\vec p=(1,1,1)$ as the kernel of an 
	affine function.
\end{exercise}

\subsection{Covectors}

We will now develop
a set of tools to \emph{measure} geometric objects.  The first
property we will attempt to measure is displacement.  Using
displacement,
we will be able to measure area as ``displacement in two directions''
and volume as ``displacement in three directions.''

Displacement is measures with objects called \emph{covectors}\index{covector}.
In other contexts, these objects are also known as 
\emph{linear functionals}\index{linear functional} or \emph{dual vectors}\index{dual vector}.
We will typically write covectors in a \textbf{bold} font to  distinguish 
them from vectors.

\begin{definition}[Covector]
	Given a vector space $V$, a \emph{covector} on the space $V$
	is a linear function $\bm{\alpha}:V\to \R$.  The set of all
	covectors on $V$ is denoted $V^*$.
\end{definition}

This definition is very abstract, but again, we've encountered covectors
before!  Let $\bm{\alpha}:\R^3\to\R$ be defined by $\bm{\alpha}(\vec v) =\xhat \cdot \vec v$.
We already know that $\bm{\alpha}$ is linear and that it outputs
real numbers.  Therefore it is a covector.  However, $\bm \alpha$ also has
geometric meaning.  It $\bm\alpha(\vec v)$ outputs the $x$-coordinate
of $\vec v$.  We could similarly come up with covectors that output the $y$ or $z$ coordinates
of their input.

Any function $\bm f:\R^n\to \R$ defined by $\bm f(\vec v) = \vec r\cdot \vec v$
for some fixed $\vec r$ is a covector.  Further, we can interpret
$\bm f$ to have geometric meaning.  The quantity $\bm f(\vec v)$ is how
much $\vec v$ points in the direction $\vec r$, scaled by $\norm{\vec r}$.  
In other words, $\bm f(\vec v)$
is the ``$\vec r$ component'' of $\vec v$ (is anyone else thinking
about projections?) which is ``the length of $\vec v$ measured by a ruler
with ticks determined by $\vec r$.''

A theorem from linear algebra shows us that this type of covector
is all there is (in $\R^n$, at least).

\begin{theorem}
	Let $\bm\alpha:\R^n\to \R$ be a covector for $\R^n$.  Then,
	there exists a $\vec a\in \R^n$ such that
	\[
		\bm\alpha (\vec v) = \vec a\cdot \vec v
	\]
	for all $\vec v\in\R^n$.
\end{theorem}

Now we know all covectors for $\R^n$ have a geometric meaning
as measuring some component of an input vector.  The following
theorem says that the term ``vector'' in co\emph{vector} is justified.

\begin{theorem}
	If $V$ is a vector space, then $V^*$, the set of all
	covectors for $V$, is also a vector space.
\end{theorem}
\begin{proof}
    Let $f,g\in V^*$ and $\alpha\in\R$. If we can show that $f+g$ and $\alpha f$ are both covectors, 
    then it will follow that $V^*$ is closed under linear combinations. By the
    argument in \ref{EXVECSPACEEXAMPLE}, both $f+g$ and $\alpha f$ are functions from
    $V$ to $\R$. Thus, we need only show that they are linear. Let $h=f+g$. Then for 
    $\vec u,\vec v\in V$, we have
    \begin{align*}
        h(\vec u+\vec v) & = f(\vec u + \vec v) + g(\vec u + \vec v) \\
                         & = f(\vec u) + f(\vec v) + g(\vec u) + g(\vec v) \\
                         & = \big(f(\vec u) + g(\vec u)\big) + \big(f(\vec v) + g(\vec v)\big) \\
                         & = h(\vec u) + h(\vec v),
    \end{align*} 
    and for any $\beta\in\R$
    \begin{align*}
         h(\beta\vec u) & = f(\beta\vec u) + g(\beta\vec u) \\
                         & = \beta f(\vec u) + \beta g(\vec u) \\
                         & = \beta \big(f(\vec u) + g(\vec u)\big) \\
                         & = \beta h(\vec u).
    \end{align*}    
    Let $q=\alpha f$. Then
    \begin{align*}
    	q(\vec u +\vec v) & = \alpha f(\vec u + \vec v) \\
                          & = \alpha f(\vec u) + \alpha f(\vec v) \\
                          & = q(\vec u) + q(\vec u),
    \end{align*} 
    and for any $\beta\in\R$ we have
    \[
        q(\beta\vec u)=\alpha f(\beta\vec u)=\alpha\beta f(\vec u) 
        =\beta\big(\alpha f(\vec u)\big)=\beta q(\vec u).
    \]
    Therefore, $V^*$ is closed under linear combinations. The associativity,
    distributivity, and commutativity properties are inherited from $\R$. 
    The identity property is satisfied by the zero covector which maps every
    element of $V$ to $0$.
\end{proof}

We won't be doing much with the abstract vector space $V^*$, but it is
nice to know that we can take linear combinations of covectors
and still get a covector\footnote{ If we wanted to investigate $V^*$,
we might start by saying, since $V^*$ is a vector space, there is
a set $V^{**}$ of covectors for $V^*$.  Another linear algebra theorem
says $V\subseteq V^{**}$, if you interpret ``subset'' in the right sense. }.

It is, however, worth our time to consider $(\R^n)^*$.  Recall that
in $\R^3$ we have special vectors $\xhat$, $\yhat$, and $\zhat$,
called the standard basis for $\R^3$,
and all other vectors can be written as a linear combination of these.
Higher dimensions also have standard bases, as do the corresponding
spaces of covectors.

\begin{definition}[Standard Basis]
	The \emph{standard basis} for $\R^n$ is notated 
	$\vec e_1,\vec e_2,\ldots,\vec e_n$. It satisfies that 
	every vector in $\R^n$ can be written as a linear combination of
	$\vec e_1,\vec e_2,\ldots,\vec e_n$ and that
	\[
		\vec e_i\cdot \vec e_j = \begin{cases}
			1 &\text{ if } i=j\\
			0 &\text{ if } i\neq j
		\end{cases}.
	\]

	The standard basis for $(\R^n)^*$ is notated
	$\bm e^1,\bm e^2,\ldots,\bm e^n$. It satisfies that
	every covector for $\R^n$ can be written as a linear combination of
	$\bm e^1,\bm e^2,\ldots,\bm e^n$ and that
	\[
		\bm e^i(\vec e_j) = \begin{cases}
			1 &\text{ if } i=j\\
			0 &\text{ if } i\neq j
		\end{cases}.
	\]
\end{definition}

Following the convention from physics, we index covectors using superscripts
and regular vectors using subscripts.  In plain terms, the standard basis for
$\R^n$ is a collection of unit vectors that point along the coordinate axes\footnote{
Technically, the standard basis \emph{defines} the coordinate axes.}, and the
standard basis for $(\R^n)^*$ picks off the standard coordinates of input
vectors.

It's now easy to appreciate that we can take linear combinations of covectors.
For example, let $\bm \alpha = \bm e^1+2\bm e^3$.  We can compute
\[
	\bm \alpha\mat{a\\b\\c} = \bm e^1\mat{a\\b\\c}
	+2\bm e^3\mat{a\\b\\c} = a+2c = \mat{1\\0\\2}\cdot \mat{a\\b\\c},
\]
and so we see that $\bm\alpha$ is given by taking the dot product with
the vector $\mat{1\\0\\2}$.  Of course, we could write $\vec e_1+2\vec e_3$
instead of $\mat{1\\0\\2}$ and $a\vec e_1+b\vec e_2+c\vec e_3$ instead of 
$\mat{a\\b\\c}$.  In doing so, we would be analyzing vectors in
a \emph{coordinate free} manner.  That is, we would never need to talk about
the coordinates of a vector in order to do a computation.  One of the selling points
of differential forms is that they allow you to do multivariable calculus
in a coordinate free way.

Stepping back for a moment, the machinery we've developed is almost comically
tautological.  Let $\vec v\in \R^n$ and let $\bm \alpha$ be a covector for $\R^n$.  By 
definition,
\begin{align*}
	\vec v &= a_1\vec e_1+\cdots +a_n\vec e_n\\
	\bm \alpha &= b_1\bm e^1+\cdots b_n\bm e^n.
\end{align*}
It follows that
\[
	\bm \alpha(\vec v) = 
	(b_1\bm e^1+\cdots b_n\bm e^n)(a_1\vec e_1+\cdots +a_n\vec e_n)
	=b_1a_1+\cdots b_na_n = \matc{b_1\\\vdots\\b_n}\cdot \matc{a_1\\\vdots\\
	a_n}.
\]
Thus, while by definition 
\[
\vec v=a_1\vec e_1+\cdots +a_n\vec e_n=
\matc{a_1\\\vdots\\a_n},\]
if we  wrote
\[
	\bm \alpha =b_1\bm e^1+\cdots b_n\bm e^n=
\matc{b_1\\\vdots\\b_n},\]
we would have gone through
a whole lot of work to reinvent the dot product!  It will be worth it 
though\footnote{ Some would argue that dot and cross products confuse
vectors and covectors---that quantities like \emph{force}
are naturally covectors, and that physics should be taught
with covectors (instead of dot products) from the beginning.}.


\subsection{Covectors as Rulers}

A covector measures a particular component of a vector (maybe scaled), and
so we could think of them as rulers.  For example, consider the covector
$\bm\alpha = \bm e^1+\bm e^2$.  The set of vectors that $\bm\alpha$ measures
to be zero is exactly the set $M_0=\Set{(x,y)\given x+y=0}$.  The set of vectors
$\bm\alpha$ measures to be $1$ is exactly the set $M_1=\Set{(x,y)\given x+y=1}$ and so on.

If we graph $M_0$, $M_1$, etc., we see that $\bm\alpha$ is laying out a ``ruler'' on $\R^2$.

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
			name=plot1,
		    anchor=origin,
		    disabledatascaling,
		    xmin=-1,xmax=3,
		    ymin=-1,ymax=2,
		    x=1cm,y=1cm,
		    grid=both,
		    grid style={line width=.1pt, draw=gray!10},
		    %major grid style={line width=.2pt,draw=gray!50},
		    axis lines=middle,
		    minor tick num=0,
		    enlargelimits={abs=0.5},
		    axis line style={latex-latex},
			%xticklabels={,,},
			%yticklabels={,,},
			xlabel={$x$},
			ylabel={$y$},
		    xlabel style={at={(ticklabel* cs:1)},anchor=west},
		    ylabel style={at={(ticklabel* cs:1)},anchor=south}
		]

	\pgfplotsinvokeforeach{-2,...,5} {
		\addplot[color=myorange, dashed, thick, domain=-3:5] ({x},{#1-x});
		\coordinate (A#1) at ({#1-1.5}, -1.5);

}

		\end{axis}

		\draw (A1) node[below] {$M_{-2}$};
		\draw (A2) node[below] {$M_{-1}$};
		\draw (A3) node[below] {$M_{0}$};
		\draw (A4) node[below] {$M_{1}$};
		\draw (A5) node[below] {$M_{2}$};
	\end{tikzpicture}
\end{center}

If we draw a vector $\vec v\in \R^2$, we can measure it with respect to $\bm\alpha$ by
looking at how many ruler marks it crosses.  For example, $\vec v=(2,1)$ lands at $M_3$, the
third ruler mark, so $\bm\alpha(\vec v) = 3$.

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
			name=plot1,
		    anchor=origin,
		    disabledatascaling,
		    xmin=-1,xmax=3,
		    ymin=-1,ymax=2,
		    x=1cm,y=1cm,
		    grid=both,
		    grid style={line width=.1pt, draw=gray!10},
		    %major grid style={line width=.2pt,draw=gray!50},
		    axis lines=middle,
		    minor tick num=0,
		    enlargelimits={abs=0.5},
		    axis line style={latex-latex},
			%xticklabels={,,},
			%yticklabels={,,},
			xlabel={$x$},
			ylabel={$y$},
		    xlabel style={at={(ticklabel* cs:1)},anchor=west},
		    ylabel style={at={(ticklabel* cs:1)},anchor=south}
		]

	\pgfplotsinvokeforeach{-2,...,5} {
		\addplot[color=myorange, dashed, thick, domain=-3:5] ({x},{#1-x});
		\coordinate (A#1) at ({#1-1.5}, -1.5);

}

		\draw[->, thick, mypink] (0,0) -- (2,1);

		\end{axis}

		\draw (A1) node[below] {$M_{-2}$};
		\draw (A2) node[below] {$M_{-1}$};
		\draw (A3) node[below] {$M_{0}$};
		\draw (A4) node[below] {$M_{1}$};
		\draw (A5) node[below] {$M_{2}$};
	\end{tikzpicture}
\end{center}

Of course, the ruler lines for $\bm\alpha$ are all parallel, so $\bm\alpha$ can
only measure ``one dimension'' of a vector.  For example, $\vec w=(4,-1)\neq \vec v$ and
$\norm{\vec v}\neq \norm{\vec w}$, but they lie on the same ruler line, and so
$\bm\alpha(\vec v) = \bm\alpha(\vec w)$.

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
			name=plot1,
		    anchor=origin,
		    disabledatascaling,
		    xmin=-1,xmax=4,
		    ymin=-1,ymax=2,
		    x=1cm,y=1cm,
		    grid=both,
		    grid style={line width=.1pt, draw=gray!10},
		    %major grid style={line width=.2pt,draw=gray!50},
		    axis lines=middle,
		    minor tick num=0,
		    enlargelimits={abs=0.5},
		    axis line style={latex-latex},
			%xticklabels={,,},
			%yticklabels={,,},
			xlabel={$x$},
			ylabel={$y$},
		    xlabel style={at={(ticklabel* cs:1)},anchor=west},
		    ylabel style={at={(ticklabel* cs:1)},anchor=south}
		]

	\pgfplotsinvokeforeach{-2,...,6} {
		\addplot[color=myorange, dashed, thick, domain=-3:5] ({x},{#1-x});
		\coordinate (A#1) at ({#1-1.5}, -1.5);

}

		\draw[->, thick, mypink] (0,0) -- (2,1);
		\draw[->, thick, mygreen] (0,0) -- (4,-1);

		\end{axis}

		\draw (A1) node[below] {$M_{-2}$};
		\draw (A2) node[below] {$M_{-1}$};
		\draw (A3) node[below] {$M_{0}$};
		\draw (A4) node[below] {$M_{1}$};
		\draw (A5) node[below] {$M_{2}$};
		\draw (A6) node[below] {$M_{3}$};
	\end{tikzpicture}
\end{center}


\section{1-forms and Line Integrals}

Recall that a function $f:\R^n\to\R^n$ is called a
vector field because to each point in $\R^n$ it assigns a
vector in $\R^n$.  A 1-form is a type of \emph{differential 
form}\index{differential form} that is analogous to a vector field
except that instead of assigning to each point a vector,
it assigns to each point a \emph{covector}.  We will also write
1-forms using a \textbf{bold} font.

\begin{definition}[1-form]
	A \emph{1-form}\index{1-form} on the space $\R^n$ is a function
	$\bm \alpha: \R^n\to (\R^n)^*$.
\end{definition}
We will always assume our 1-forms are \emph{smooth}---that is, every 1-form
can be differentiated as much as we like\footnote{ We haven't defined
what it means to differentiate a function whose codomain consists of covectors.
The quick and dirty answer is, treat covectors as vectors and use
the definition of differentiability for a vector field.}.

1-forms output covectors which themselves measure vectors.
Thus in order to get a number from a 1-form, there is a chain
of function applications.  We will introduce some notation to make this
tolerable.  If $\bm \alpha$ is a 1-form, then
$\bm \alpha(\vec v)$ is a covector.  If we wish to apply this
covector, $\bm\alpha(\vec v)$,
to measure the vector $\vec w$, we write $\Big[\bm\alpha(\vec v)\Big](\vec w)$.
We'll be doing this a lot, so we have the alternative notation
\[
	\bm\alpha\formarg{\vec v}{\vec w}  \equiv\Big[\bm\alpha(\vec v)\Big](\vec w).
\]


As we've seen, covectors are used to measure oriented line
segments (vectors).  1-forms measure \emph{oriented curves}.  We'll explore this by
first considering some special 1-forms on $\R^3$.

\begin{definition}[Standard Basis for 1-forms]
	The
	\emph{standard basis for 1-forms on $\R^3$} are the 1-forms
	$\dd x$, $\dd y$, and $\dd z$ defined by
	\[
		\dd x(\vec v) = \bm e^1\qquad \dd y(\vec v)=\bm e^2\qquad
		\dd z(\vec v) = \bm e^3
	\]
	for all $\vec v\in \R^3$.
\end{definition}

The names $\dd x$, $\dd y$, and $\dd z$\index{$\dd x$ $\dd y$ $\dd z$}
are certainly evocative.  When you see
``$\dd$,'' you should think ``displacement.''  After all
$\dd x(\vec v)$ is the covector that measures the $x$ displacement
of a vector.  Soon, you will
also think ``derivative.''

\subsection{Measuring Curves with 1-forms}

Let $\mathcal C\subset \R^3$ be a smooth curve that starts at $\vec a=(a_x,a_y,a_z)$
and ends at $\vec b=(b_x,b_y,b_z)$.  The 1-form $\dd x$ is meant to measure the
displacement of $\mathcal C$ from it's starting point to its ending point.
However, $\dd x$ cannot \emph{directly} interact with $\mathcal C$.
Recall, $\dd x:\R^3\to (\R^3)^*$ is a function that inputs points
and outputs covectors and covectors measure vectors.  $\mathcal C$ certainly
has points, and if we squint our eyes, $\mathcal C$ also has vectors---tangent
vectors!

Let $\vec r:[0,1]\to\mathcal C$ be a parameterization of $\mathcal C$ defined by
\[
\vec r(t) = \mat{r_x(t)\\r_y(t)\\r_z(t)}
\]
so that $\vec r(0)=\vec a$ and $\vec r(1)=\vec b$ (that is, it is oriented 
appropriately). Then, 
\[\vec r'(t) =\mat{r_x'(t)\\r_y'(t)\\r_z'(t)}
\]is a tangent vector to $\mathcal C$ at the
point $\vec r(t)$.

\begin{center}
	\begin{tikzpicture}[
    tangent/.style={
        decoration={
            markings,% switch on markings
            mark=
                at position #1
                with
                {
                    \coordinate (tangent point-\pgfkeysvalueof{/pgf/decoration/mark info/sequence number}) at (0pt,0pt);
                    \coordinate (tangent unit vector-\pgfkeysvalueof{/pgf/decoration/mark info/sequence number}) at (1,0pt);
                    \coordinate (tangent orthogonal unit vector-\pgfkeysvalueof{/pgf/decoration/mark info/sequence number}) at (0pt,1);
                }
        },
        postaction=decorate
    },
    use tangent/.style={
        shift=(tangent point-#1),
        x=(tangent unit vector-#1),
        y=(tangent orthogonal unit vector-#1)
    },
    use tangent/.default=1
]
		\begin{axis}[
			name=plot1,
		    anchor=origin,
		    disabledatascaling,
		    xmin=-1,xmax=4,
		    ymin=-1,ymax=2,
		    x=1cm,y=1cm,
		    grid=both,
		    grid style={line width=.1pt, draw=gray!10},
		    %major grid style={line width=.2pt,draw=gray!50},
		    axis lines=middle,
		    minor tick num=0,
		    enlargelimits={abs=0.5},
		    axis line style={latex-latex},
			%xticklabels={,,},
			%yticklabels={,,},
			xlabel={$x$},
			ylabel={$y$},
		    xlabel style={at={(ticklabel* cs:1)},anchor=west},
		    ylabel style={at={(ticklabel* cs:1)},anchor=south}
		]

	\pgfplotsinvokeforeach{-2,...,6} {
		\addplot[color=myorange, densely dashed, thick, domain=-3:5] ({#1},{x});
}
	\pgfplotsinvokeforeach{-1.5,...,6} {
		\addplot[color=myorange, dotted, thin, domain=-3:5] ({#1},{x});
}


		

		\end{axis}

       %\draw[thick, red, fill=red!30] (3,-1) .. (2,0) .. (0,0.93) .. (-0.93,0) .. (0,-0.93) .. (2,0) .. (3,1);

		\draw[tangent=.0, tangent=.2, tangent=.4, tangent=.6, tangent=.8,tangent=.99,mypink, thick] (.5,.5) to[curve through={(2,0) .. (1.7,1)}] (4,2);
		\foreach \i in {1, ..., 6} {
			\draw[use tangent=\i, -stealth, blue] (0,0) -- (.7,0);
		}
		\draw[draw=none, fill=black] (.5,.5) circle (1.5pt) node[right] {$\vec a$};
		\draw[draw=none, fill=black] (4,2) circle (1.5pt) node[right] {$\vec b$};
		\draw[draw=none, fill=black] (1,2.5) node[above, myorange] {$\dd x$ ruler};

	\end{tikzpicture}
\end{center}

Now, $\dd x\circ \vec r = \bm e^1$ is the constant covector that measures
the $x$ displacement of a vector and so $\dd x \formarg{\vec r(t)}{\vec r'(t)}
=\bm e^1\big(\vec r'(t)\big)$
is the $x$ displacement of $\vec r'(t)$.   In other words,
\[
	\dd x\formarg{\vec r(t)}{\vec r'(t)}=\bm e^1\big(\vec r'(t)\big) = r_x'(t).
\]
What we're after is  $b_x-a_x$, the \emph{total} $x$ displacement along the curve $\mathcal C$.
We  can get this quantity from the fundamental theorem
of calculus.  After some reverse engineering, we see
\[
	b_x-a_x = \int_0^1 r_x'(t)\, \d t 
	= \int_0^1 \dd x\formarg{\vec r(t)}{\vec r'(t)}\,\d t.
\]
Further, this integral works out to the same value regardless of our
choice of parameterization.  If $\vec q$ were another parameterization 
of $\mathcal C$ starting
at $\vec a$ and ending at $\vec b$, then 
\[
\int_0^1 \dd x\formarg{\vec q(t)}{\vec q'(t)}\,\d t=b_x-a_x.
\]
Based on this observation,
we are ready to define what it means to integrate a 1-form.

\begin{definition}[Integral of a 1-form]
	Let $\mathcal C$ be an oriented curve that starts at $\vec a$
	and ends at $\vec b$ and let $\bm \alpha$ be a 1-form.  We define
	the \emph{integral of $\bm \alpha$ over the curve $\mathcal C$}
	by
	\[
		\int_{\mathcal C}\bm\alpha 
		=\int_a^b \bm\alpha \formarg{\vec q(t)}{\vec q'(t)}\,\d t
		= \int_a^b \Big[\bm\alpha\circ \vec q(t)\Big](\vec q'(t))\,\d t
	\]
	where $\vec q:[a,b]\to\mathcal C$ is a parameterization of
	$\mathcal C$ satisfying $\vec q(a)=\vec a$ and $\vec q(b)=\vec b$.
\end{definition}

With this definition, we have a powerful notation.  For an oriented curve $\mathcal C$,
\[
	\int_{\mathcal C} \dd x
\]
is the $x$ displacement from the start of $\mathcal C$ to the end of $\mathcal C$.
Similarly, $\int_{\mathcal C}\dd y$ gives the $y$ displacement and $\int_{\mathcal C}\dd z$
gives the $z$ displacement.
Further, the integral notation for 1-forms
behaves like a classical integral with respect to linear combinations.
If $\bm\alpha$ and $\bm\beta$ are 1-forms and $t$ is a scalar, then
\[
	\int_{\mathcal C}(\bm \alpha + t\bm\beta) = \int_{\mathcal C} \bm\alpha 
	+ t\int_{\mathcal C} \bm\beta.
\]

\begin{exercise}
	Explain in words how to interpret
	the output of $\int_{\mathcal C} (\dd x+\dd y +\dd z)$.
\end{exercise}


\subsection{Non-constant 1-forms}

Let us seek the comfort of a one dimensional space as we explore non-constant 1-forms.
In $\R^1$, we only have one standard basis 1-form: $\dd x$.
Let $f:\R^1\to\R$ be a scalar-valued function.  We will interpret
$\R^1$ as a vector space rather than a set of scalars. Now, define
\[
	f\dd x (\vec v) = f(\vec v)\dd x(\vec v) = f(\vec v)\bm e^1.
\]
That is, $f\dd x$, when given a point, returns a scaled version of $\bm e^1$,
which will in turn measure a scaled version of the $x$ coordinate of
a vector.  Let $\mathcal C=[a,b]$ be the oriented interval starting
at $a$ and ending at $b$.  $\mathcal C$ is naturally parameterized
by the function $\vec r:[a,b]\to[a,b]$ defined by $\vec r(t) = t$.
Since $\vec r$ is the identity function, $f\circ \vec r(t)=f(t)$ and
$\vec r'(t) = 1$.
Thus, we may compute
\[
	\int_{\mathcal C} f\dd x = \int_a^b f\circ r(t)\, \dd x\formarg{\vec r(t)}{\vec r'(t)}
	\,\d t
	=\int_a^b f(t)\,\d t.
\]
This shows that
$\int_{\mathcal C} f\dd x$ is the usual integral of $f$ on the interval $[a,b]$.

\bigskip
Moving to $\R^2$, let $f_x:\R^2\to\R$ and $f_y:\R^2\to\R$
and consider the 1-form $\bm\alpha = f_x\dd x+f_y\dd y$.  Again, let
$\mathcal C$ be an oriented curve starting at $\vec a$, ending at $\vec b$,
and parameterized by $\vec r:[0,1]\to\mathcal C$  where
$\vec r(t) = (r_x(t),r_y(t))$ and  $\vec r(0)=\vec a$ and
$\vec r(1)=\vec b$. 

Now,
\begin{align*}
	\int_{\mathcal C} \bm\alpha &= 
	\int_{\mathcal C} f_x\dd x+\int_{\mathcal C} f_y\dd y\\
	&=\int_0^1 f_x\circ \vec r(t)\, \dd x\formarg{\vec r(t)}{\vec r'(t)}\,\d t
	+\int_0^1 f_y\circ \vec r(t)\, \dd y\formarg{\vec r(t)}{\vec r'(t)}\,\d t\\
	&= \int_0^1 f_x\circ \vec r(t)\, r_x'(t)\,\d t
	+\int_0^1 f_y\circ \vec r(t)\, r_y'(t)\,\d t
\end{align*}
If we let $\vec F = (f_x,f_y)$, a familiar integral emerges.
\[
	\int_{\mathcal C} \bm\alpha = \int_0^1 \vec F\circ \vec r(t) \cdot \vec r'(t)\,\d t
	=\int_{\mathcal C} \vec F\cdot \d\vec r,
\]
where the right-hand side is the vector line integral of $\vec F$ along the curve
$\mathcal C$.  So, integrals of 1-forms along curves are actually vector line
integrals.

\begin{example}
	Let $\bm \alpha=-y\dd x+x\dd y$ and let $\mathcal C$ be the circle of
	radius 2 centered at the origin oriented counter-clockwise.  Compute $\int_C \bm\alpha$.

	We can parameterize $\mathcal C$ by $\vec r(t) = (2\cos t,2\sin t)$.  Pre-computing,
	$\vec r'(t) = (-2\sin t,2\cos t)$ and so,
	\begin{align*}
		\bm\alpha\formarg{\vec r(t)}{\vec r'(t)} &= -2\sin t\, \dd x\formarg{\vec r(t)}{\vec r'(t)}
		+2\cos t\,\dd y\formarg{\vec r(t)}{\vec r'(t)}\\
		&= 4\sin^2 t+4\cos^2 t\\
		&= 4.
	\end{align*}
	Thus,
	\[
		\int_C \bm\alpha = \int_0^{2\pi} 4 \,\d t = 8\pi.
	\]
\end{example}

\section{2-forms and Surface Integrals}

\subsection{The Wedge Product}


We can think of covectors as measuring oriented line segments.  That is,
any vector you can think of as a line segment with a start and an end.
When the covector $\bm\alpha=a\bm e^1+b\bm e^2+c\bm e^3$ is applied to 
a vector $\vec v$, it measures the ``$(a,b,c)$ component'' of $\vec v$.
In other words, it measures how long $\vec v$ appears to be from the perspective
of the vector $(a,b,c)$.

We can invent a similar object that measures oriented parallelograms.
Any pair of vectors $(\vec v,\vec w)$ can be viewed as forming an oriented
parallelogram $\mathcal P_{\vec v,\vec w}$.  If $(\vec v,\vec w)$ come in a
right-handed order, we say $\mathcal P_{\vec v,\vec w}$ has a positive
orientation.  Otherwise, it has a negative orientation.
A function
that takes in a pair of vectors and measures the oriented (signed)
area of the parallelogram they determine is called a 2-covector.

Let us take a moment to discuss the \emph{laws of area}\footnote{ We'll be
calling them \emph{laws of volume} later on.} that should
apply to 2-covector.  If $\bm\alpha$ is a 2-covector and $\vec v,\vec w$ are
vectors, then
\[
	\bm\alpha(\vec v,\vec v) = 0,
\]
since the parallelogram $\mathcal P_{\vec v,\vec v}$ is degenerate\footnote{
This isn't a comment about the parallelogram's moral character; it just has no area.}.  Scaling one of the sides should affect
the area accordingly:
\[
	\bm\alpha(m\vec v,\vec w) = m\alpha(\vec v,\vec w) = \alpha(\vec v,m\vec w).
\]
Further, if we slide one vector parallel to the other, it shouldn't affect
the area.  So,
\[
	\bm\alpha(\vec v,\vec w) = \alpha(\vec v+m\vec w,\vec w) = \alpha(\vec v,\vec w+m\vec v).
\]

Lastly, though not a direct consequence of the laws of area,
we will require that $\bm\alpha$ is linear in each argument.
That is, that $\bm\alpha$ is \emph{multi-linear}\index{multi-linear}.

\begin{definition}[Multi-linear]
	A function $f:\underbrace{V\times \cdots\times V}_{n}\to W$ is
	\emph{multi-linear} if it is linear in each argument.
	That is, for all $\vec v_1,\ldots, \vec v_n,\vec w\in V$ and 
	all scalars $\alpha$,
	\[
		f(\vec v_1,\ldots,\vec v_i+\vec w,\ldots,\vec v_n)
		=
		f(\vec v_1,\ldots,\vec v_i,\ldots,\vec v_n) +
		f(\vec v_1,\ldots,\vec w,\ldots,\vec v_n)
	\]
	and
	\[
		f(\vec v_1,\ldots,\alpha\vec v_i,\ldots,\vec v_n)
		=
		\alpha f(\vec v_1,\ldots,\vec v_i,\ldots,\vec v_n).
	\]
\end{definition}

\begin{definition}[2-covector]
	A \emph{2-covector} on the vector space $V$ is a multi-linear
	function $\bm\alpha: V\times V\to \R$ that obeys the laws of area.
\end{definition}

The laws of area should look very familiar from our 
derivation of the cross product.  And, now we can see that orientation
is a direct consequence of multi-linearity and the fact that $\bm\alpha(\vec v,\vec v)=0$.
Observe,
\begin{align*}
	0=\bm\alpha(\vec v+\vec w, \vec v+\vec w) &= 
		\bm\alpha(\vec v, \vec v)+
		\bm\alpha(\vec v, \vec w)+
		\bm\alpha(\vec w, \vec v)+
		\bm\alpha(\vec w,\vec w)\\
		&=\bm\alpha(\vec v, \vec w)+
		\bm\alpha(\vec w, \vec v),
\end{align*}
and so $\bm\alpha(\vec v, \vec w) = -\bm\alpha(\vec w, \vec v)$.  This property
is called \emph{alternating}\index{alternating map}.

We've generated a lot of theory. Let's see an actual 2-vector in action.

\begin{example}
	\label{EXTWOCOVECTOR}
	For a pair of vectors $(\vec v,\vec w)$, let $\mathcal P_{\vec v,\vec w}$
	be the oriented parallelogram with sides $\vec v$ and $\vec w$.
	Define $\bm\alpha$ to be the function such that $\bm\alpha(\vec v,\vec w)$
	is the oriented area of $\mathcal P_{\vec v,\vec w}$ projected onto the
	$xy$-plane.  Show that $\bm\alpha$ is a 2-covector.

	\begin{center}
	\begin{tikzpicture}
	    \begin{axis}[grid=major,view={15}{25}, xmin=-1, xmax=6, ymin=-1, ymax=4,zmin=0,
		    colormap/viridis,
		    scale mode=scale uniformly,
xticklabels={,,}, yticklabels={,,}, zticklabels={,,},
			name=plot0
		    ]

		\addplot3[draw=none]  (0,0,0);

		\coordinate (O) at (0,0,0);
		\coordinate (A) at (1,2,1);
		\coordinate (B) at (4,0,3);
		\coordinate (C) at (3,-2,2);

		\draw[fill opacity = 0.05, fill=blue] (O) -- (A) -- (B) -- (C) --cycle;
		\draw[fill opacity=0.1, fill=mygreen] (0,0,0) -- (1,2,0) -- (4,0,0) -- (3,-2,0) --cycle;
		\draw[fill=black, draw=none](0,0,0) circle (1.5pt);
		\draw[->,thick, myorange] (0,0,0) -- (1,2,1);
		\draw[->,thick, myorange] (0,0,0) -- (3,-2,2);

		\draw[densely dashed] (1,2,0) -- (A) (4,0,0) -- (B) (3,-2,0) -- (C);
		\coordinate (X) at (3,0,0);
		%\draw (0,0,0) -- (1,-2,-4);

	    \end{axis}

	\draw (plot0.south) node[below] {Projected area} edge[bend right, ->] (X);

	  \end{tikzpicture}
	\end{center}

	Let $\vec v=(v_x,v_y,v_z)$ and $\vec w=(w_x,w_y,w_z)$ be the components
	of $\vec v$ and $\vec w$.  Using components, the projection of each
	vector onto the $xy$-plane is given by $(v_x,v_y,0)$ and $(w_x,w_y,0)$.
	We can compute the oriented area of this parallelogram in the $xy$-plane
	using the cross product, which
	results in the formula
	\[
		\bm\alpha(\vec v,\vec w) = v_xw_y-v_yw_x.
	\]
	It is left to the reader to verify that $\bm\alpha$ satisfies the area
	laws and is multi-linear.
\end{example}

The 2-covector $\bm\alpha(\vec v,\vec w) = v_xw_y-v_yw_x$ from Example \ref{EXTWOCOVECTOR}
is interesting because it only relies on the $x$ and $y$ components of its
input vectors. And, the covectors $\bm e^1$ and $\bm e^2$ measure the $x$ and
$y$ components of their input!  Thus, we could write
\[
	\bm\alpha(\vec v,\vec w) = \bm e^1(\vec v)\bm e^2(\vec w) - \bm e^2(\vec v)\bm{e}^1(\vec w).
\]
There's a beautiful symmetry to that formula---so beautiful that it is encapsulated
in a concept called the \emph{wedge product}\index{wedge product}.

\begin{definition}[Wedge Product]
	Let $\bm \alpha$ and $\bm \beta$ be covectors.  The \emph{wedge product}
	(also known as the \emph{exterior product}\index{exterior product})
	is notated $\bm\alpha\wedge\bm\beta$ and is defined by
	\[
		\bm\alpha\wedge\bm\beta (\vec v,\vec w)
		=\bm\alpha(\vec v)\bm\beta(\vec w) - \bm\beta(\vec v)\bm\alpha(\vec w).
	\]
\end{definition}

\begin{theorem}
	If $\bm \alpha$ and $\bm \beta$ be covectors then the wedge
	product $\bm\alpha\wedge\bm\beta$ is a 2-covector.
\end{theorem}
\begin{proof}
 	XXX Finish
\end{proof}

We now see that our area-in-the-$xy$-plane 2-covector from before could
be written as $\bm e^1\wedge \bm e^2$.  But, it gets better.

\begin{theorem}
	Every 2-covector can be represented as a linear combination
	of vectors in the set $\Set{\bm e^i\wedge \bm e^j\given i\neq j}$.
\end{theorem}

In general, it is not true that if $\bm\alpha$ is a 2-covector
then $\bm\alpha$ can be written as the wedge product of two covectors\footnote{
This \emph{does} happen to be true for 2-covectors on $\R^3$, but that's just
a happy coincidence.}.  
We really do need to allow linear combinations to get 
all 2-covectors.  For example, the 2-covector $\bm e^1\wedge\bm e^2+\bm e^3\wedge
\bm e^4$ for $\R^4$ cannot be written as the wedge product of two covectors.

\bigskip
Exploring the consequences of the definition of the wedge product, 
we see that the wedge product is anti-symmetric:
\[
	\bm\alpha\wedge \bm \beta = -\bm\alpha\wedge \bm\beta.
\]
And, that it is multi-linear:
\[
	(m\bm\alpha)\wedge \bm \beta = m(\bm\alpha\wedge \bm \beta)
	=\bm\alpha\wedge (m \bm \beta)
\]
\[
	(\bm\alpha+\bm\gamma)\wedge \bm\beta = \bm\alpha\wedge\bm\beta + \bm\gamma\wedge \bm\beta
\]
\[
	\bm\alpha\wedge(\bm\gamma+\bm\beta) = \bm\alpha\wedge\bm\gamma + \bm\alpha\wedge\bm\beta.
\]


\subsection{Geometric Interpretation of 2-covectors}
Like covectors measure components of the length of a vector,
2-covectors measure components of the area of a parallelogram determined by pairs of vectors.
In particular, in $\R^3$, the standard basis covectors $\bm e^1\wedge \bm e^2$, $\bm e^2\wedge \bm e^3$,
and $\bm e^3\wedge \bm e^1$ measure the oriented area of a parallelogram projected onto the $xy$, $yz$,
and $xz$ planes.

\begin{center}
	\begin{tikzpicture}
	    \begin{axis}[grid=major,view={10}{25}, xmin=-1, xmax=6, ymin=-2.5, ymax=2.5,zmin=-1, zmax=4,
		    colormap/viridis,
		    %scale mode=scale uniformly,
xticklabels={,,}, yticklabels={,,}, zticklabels={,,},
			name=plot0
		    ]

		\addplot3[draw=none]  (0,0,0);

		\coordinate (O) at (0,0,0);
		\coordinate (A) at (1,2,1);
		\coordinate (B) at (4,0,3);
		\coordinate (C) at (3,-2,2);


		\draw[fill opacity=0.1, fill=mygreen] (0,0,-1) -- (1,2,-1) -- (4,0,-1) -- (3,-2,-1) --cycle;
		\draw[fill opacity=0.1, fill=mypink] (0,2.5,0) -- (1,2.5,1) -- (4,2.5,3) -- (3,2.5,2) --cycle;
		\draw[fill opacity=0.1, fill=myorange] (-1,0,0) -- (-1,2,1) -- (-1,0,3) -- (-1,-2,2) --cycle;
		\draw[fill opacity = 0.05, fill=blue] (O) -- (A) -- (B) -- (C) --cycle;
		\draw[fill=black, draw=none](0,0,0) circle (1.5pt);
		\draw[->,thick, myorange] (0,0,0) -- (1,2,1);
		\draw[->,thick, myorange] (0,0,0) -- (3,-2,2);

		\coordinate (X) at (2,0,-1);
		\coordinate (Y) at (-1,0,1.5);
		\coordinate (Z) at (2,2.5,1.5);

	    \end{axis}

		\draw (plot0.south) node[below, yshift=-.5cm] {area $\bm e^1\wedge \bm e^2$ measures} edge[bend right, ->] (X);
		\draw (plot0.north) node[below, yshift=1cm] {area $\bm e^3\wedge \bm e^1$ measures} edge[bend right, ->] (Z);
		\draw (plot0.west) node[left, yshift=1cm] {area $\bm e^2\wedge \bm e^3$ measures} edge[bend right, ->] (Y);
	  \end{tikzpicture}
\end{center}

In general, a 2-covector may be a linear combination of basis 2-covectors.  This combination may be complicated
and not correspond to measuring area projected onto any one plane.  However, as a happy coincidence of $\R^3$,
every 2-covector in $\R^3$ is equivalent to measuring the signed area of a parallelogram projected
onto some plane and then (possibly) scaling the result\footnote{ This coincidence of $\R^3$ is the same
coincidence that allows the cross product to exist.}.

\subsection{2-forms}

Now that we have 2-covectors, we can define 2-forms\index{2-form}.

\begin{definition}[2-form]
	A \emph{2-form} on the space $\R^n$ is a function $\bm\alpha:
	\R^n\to \Set{\text{2-covectors for }\R^n}$.
\end{definition}

Just as the wedge product of
two covectors produces a 2-covector, the wedge
product of two 1-forms produces a 2-form.
2-forms in $\R^3$ also have a standard basis.

\begin{definition}[Standard Basis for 2-forms]
	The standard basis for 2-forms on $\R^3$ are the 2-forms
	$\dd x\wedge \dd y$, $\dd y\wedge \dd z$, and $\dd z \wedge \dd x$.
\end{definition}

Notice the order of $\dd x$, $\dd y$, and $\dd z$.  In particular, the third basis element
is $\dd z\wedge \dd x$ and not $\dd x\wedge \dd z$.  This has to do with 
orientation.  When we use the basis 2-forms to measure area, we'd that either all of
$\dd x\wedge \dd y(\mathcal P), \dd y\wedge \dd z(\mathcal P), \dd z\wedge \dd x(\mathcal P)\geq 0$
or all of
$\dd x\wedge \dd y(\mathcal P), \dd y\wedge \dd z(\mathcal P), \dd z\wedge \dd x(\mathcal P)\leq 0$.
We never want a mix of signs.

We call $\dd x\wedge \dd y$, $\dd y\wedge \dd z$, and $\dd z \wedge \dd x$
the standard basis of 2-forms for $\R^3$ because any 2-form in $\R^3$ can
be expressed as a linear combination of 
$\dd x\wedge \dd y$, $\dd y\wedge \dd z$, and $\dd z \wedge \dd x$.

\begin{example}
	Let $\bm \alpha = x\dd x+3y^2\dd z$ and $\bm\beta = -\dd x+2\dd y -3\dd z$.
	Express $\bm \alpha \wedge \bm \beta$ as a linear combination of
	$\dd x\wedge \dd y$, $\dd y\wedge \dd z$, and $\dd z \wedge \dd x$.

	Computing,
	\begin{align*}
		\bm\alpha\wedge\bm\beta &= (x\dd x+3y^2\dd z)\wedge(-\dd x+2\dd y -3\dd z) \\
		&= -x\dd x\wedge \dd x + 2x\dd x\wedge \dd y -3x \dd x\wedge \dd z \\
		&\phantom{=-}-3y^2\dd z\wedge \dd x
			+6y^2\dd z\wedge \dd y -9y^2\dd z\wedge \dd z \\
		&=2x\dd x\wedge \dd y -3x \dd x\wedge \dd z -3y^2\dd z\wedge \dd x
			+6y^2\dd z\wedge \dd y\\
		&= 2d\dd x\wedge\dd y -6y^2\dd y\wedge \dd z + 3(x-y^2)\dd z\wedge \dd z.
	\end{align*}
\end{example}

\subsection{Integrating 2-forms}

1-forms are integrated over oriented curves.  2-forms are integrated over oriented
surfaces, and the definition of the integral of a 2-form
is nearly identical to that of a 1-form---cumbersome notation and all.

Recall, a 2-form inputs a point and outputs a 2-covector.
In turn, the 2-covector inputs a pairs of vectors and outputs a scalar.  If
$\bm \alpha$ is a 2-form, we will define
\[
	\bm\alpha\formarg{\vec x}{\vec v,\vec w}\equiv
	\Big[\bm\alpha(\vec x)\Big](\vec v,\vec w)
\]
for vectors $\vec v,\vec w$ and point $\vec x$.

\begin{definition}[integral of a 2-form]
	Let $\mathcal S$ be an oriented surface. The integral of
	the 2-form $\bm\alpha$ over $\mathcal S$ is defined to be
	\[
		\int_{\mathcal S} \bm\alpha 
		=\int_a^b\int _c^d \bm\alpha
		\formarg{\vec p(t,s)}{\tfrac{\partial \vec p}{\partial x}(t,s),
		\tfrac{\partial\vec p}{\partial y}(t,s)}\,\d s \d t,
	\]
	where $\vec p:[a,b]\times [c,d]\to\mathcal S$ in an 
	orientation-preserving parameterization of $\mathcal S$.
\end{definition}

The integral of a 2-form over a surface $\mathcal S$
doesn't depend on your choice of how you parameterize $\mathcal S$.

\begin{example}
	\label{EXTWOFORMINT}
	Let $\mathcal S$ be the region in the $xy$-plane below 
	the curve $y=1$ and above the curve $y=x^2$ oriented upward.
	Compute $\int_{\mathcal S} \dd x\wedge \dd y$.

	First, we need to parameterize this region.  Let 
	$\vec p:[-1,1]\times[0,1]\to\mathcal S$ by
	\[
		\mat{a\\b}\mapsto \mat{a\\ (1-b)a^2+b}.
	\]

	To convince ourselves that $\vec p$ actually is a parameterization,
	let's graph some curves $\vec p(t,b)$ where $b$ is held constant and
	$\vec p(a,s)$ where $a$ is held constant.

	\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
		    anchor=origin,
		    set layers=standard,
		    disabledatascaling,
		    xmin=-1,xmax=1,
		    ymin=-0,ymax=1,
		    x=2.5cm,y=2.5cm,
		    grid=both,
		    grid style={line width=.1pt, draw=gray!10},
		    %major grid style={line width=.2pt,draw=gray!50},
		    axis lines=middle,
		    minor tick num=0,
		    enlargelimits={abs=0.2},
		    axis line style={latex-latex},
		    ticklabel style={font=\tiny,fill=white},
		    xlabel style={at={(ticklabel* cs:1)},anchor=north west},
		    ylabel style={at={(ticklabel* cs:1)},anchor=south west}
		]

	    	\addplot[name path=f,no marks,mypink,thick,domain=-1:1, samples=25, draw=none] ({x},{x*x});
	    	\addplot[no marks,mypink,thick,domain=-2:2, samples=25,smooth] ({x},{x*x});
	    	\addplot[name path=g, no marks,green!50!black,thick,domain=-3:3, samples=2] ({x},{1});
		\addplot [thick, color=blue, fill=blue, fill opacity=0.25] fill between [of=f and g, split];

		\foreach \i in {0,.1,...,1} {
			\addplot[domain=-1:1, white] ({x},{(1-\i)*x^2+\i});
		}
		\foreach \i in {-1,-.8,...,1} {
			\addplot[domain=0:1, white] ({\i},{(1-x)*(\i)^2+x});
		}

	    	\addplot[no marks,mypink,thick,domain=-2:2, samples=25,smooth] ({x},{x*x});
	    	\addplot[name path=g, no marks,green!50!black,thick,domain=-3:3, samples=2] ({x},{1});

		\end{axis}
	\end{tikzpicture}
	\end{center}

	Computing, 
	\[
	\frac{\partial \vec p}{\partial x} (a,b) = \mat{1\\ 2(1-b)a} \qquad
	\frac{\partial \vec p}{\partial y} (a,b) = \mat{0\\ 1-a^2}.
	\]
	From the definition of $\dd x\wedge \dd y$,
	\[
		\dd x\wedge \dd y\formarg{\vec x}{\vec u,\vec v}
		=\bm e^1\wedge \bm e^2(\vec u,\vec v)
		= \bm e^1(\vec u)\bm e^2(\vec v) - \bm e^1(\vec v)\bm e^2(\vec u),
	\]
	and so
	\begin{align*}
	\dd x\wedge \dd y\formarg{\vec p(a,b)}{\tfrac{\partial \vec p}{\partial x}(a,b),
		\tfrac{\partial\vec p}{\partial y}(a,b)}
		&=(1)(1-a^2) - (0)(2(1-b)a^2)\\ &= 1-a^2.
	\end{align*}
	Thus, 
	\[
		\int_{\mathcal S} \dd x\wedge \dd y = \int_{-1}^1\int_0^1
		1-t^2\,\d s\d t = \tfrac{4}{3},
	\]
	which is the area of $\mathcal S$.
\end{example}

You should notice some similarities between integrating a 2-form and surface
integrals.  When computing surface integrals, once we had a parameterization
of a surface, we exploited the canonical
parameterization of the tangent plane at a point on the surface to find
a volume form (surface area element).  That's actually what happened in
Example \ref{EXTWOFORMINT}.  Let $\vec p(a,b)=(a, (1-b)a^2+b)$ be the parameterization
from Example \ref{EXTWOFORMINT}.  Then, the canonical
parameterization of the tangent plane to $\mathcal S$ at the point $(a,b)$ is
\[
	\vec L_{\vec p}(t,s) = t\frac{\partial \vec p}{\partial x}(a,b)+
		s\frac{\partial\vec p}{\partial y}(a,b) + \vec p(a,b).
\]
If we compute the area of $\vec L_{\vec p}([a,a+\Delta a]\times [b,b+\Delta b])$,
we find
\[
	\text{area of }\vec L_{\vec p}([a,a+\Delta a]\times [b,b+\Delta b])
	= (1-a^2)\Delta a\Delta b,
\]
and so the volume form associated with this parameterization would be \[\d V(t,s) 
= (1-t^2)\d t\d s.\]
This is not a coincidence.  The word \emph{form} in the term ``volume form''
is the same as in the term ``2-form.''

The techniques used in Example \ref{EXTWOFORMINT} will work in any dimension
to integrate a 2-form over any surface, whether situated in the $xy$-plane or not.
However, it can be hard to come up
with parameterizations for strange surfaces---especially with rectangular domains.
Fortunately, by adjusting the bounds of integration, you can all the domain
of your parameterization to be any reasonable subset of $\R^2$.

\begin{example}
	Let $\mathcal R$ be the region below the curve $y=1$ and above the
	curve $y=x^2$.  Compute $\int_{\mathcal R} \dd x\wedge \dd y$.

	Let $\vec p:\mathcal R\to\mathcal R$ be the identity parameterization
	given by $\vec p(a,b)=(a,b)$.  Now,
	\[
	\frac{\partial \vec p}{\partial x} (a,b) = \mat{1\\ 0}=\xhat \qquad
	\frac{\partial \vec p}{\partial y} (a,b) = \mat{0\\ 1}=\yhat.
	\]
	A little computation shows that $\dd x\wedge \dd y\formarg{\vec p(a,b)}{
	\tfrac{\partial \vec p}{\partial x} (a,b), \tfrac{\partial \vec p}{\partial y} (a,b)}
	= 1$.  Since we can explicitly define $\mathcal R$ as
	\[
		\mathcal R = \Set{(x,y)\given -1\leq x\leq 1\text{ and }x^2\leq y\leq 1},
	\]
	we may write
	\[
		\int_{\mathcal R} \dd x\wedge \dd y = \int_{-1}^1 \int_{x^2}^1
		1\,\d s\d t = \tfrac{4}{3}.
	\]
\end{example}

\subsection{Non-constant 2-forms}

Just like integrating $\dd x$ over a curve $\mathcal C$
gave the change in $x$-coordinate from the start to the end of $\mathcal C$,
integrating the 2-form $\dd x\wedge \dd y$ over the surface $\mathcal S$ will
give the signed area of $\mathcal S$ projected onto the $xy$-plane.
The integral of a non-constant 1-form over a curve
is equivalent to a vector line integral.  Analogously, the integral
of a non-constant 2-form over a surface turns out
to be a flux integral.

\begin{theorem}
	\label{THMFLUXINT}
	Let $\mathcal S\subset \R^3$ be an oriented surface in $\R^3$ and
	let $\vec F=(f_x,f_y,f_z)$ be a vector field in $\R^3$.  Then
	\[
		\Flux_{\mathcal S} \vec F
		=\int_{\mathcal S} f_x\dd y\wedge \dd z+f_y \dd z\wedge \dd x
		+f_z \dd x\wedge \dd y.
	\]
\end{theorem}

The proof of Theorem \ref{THMFLUXINT} amounts to careful bookkeeping, but it
reveals an interesting fact.  There is a correspondence between
$a \dd y\wedge \dd z+b\dd z\wedge \dd x+c\dd x\wedge \dd y$
and the vector $(a,b,c)$.  That is,
\begin{alignat*}{4}
	\dd y&\wedge &&\dd z\quad&&\text{ acts like } \quad&&\xhat \\
	\dd z&\wedge &&\dd x\quad&&\text{ acts like } \quad&&\yhat \\
	\dd x&\wedge &&\dd y\quad&&\text{ acts like } \quad&&\zhat 
\end{alignat*}
at least when thinking about flux integrals.  We will explore this
connection further when we discuss the \emph{Hodge dual}\index{Hodge dual
operator}.

\section{Higher-order Forms}

We will start by defining $n$-covectors after a minor extension
of the laws of area.  A $3$-covector is an object that measures
volume, a $4$-covector measures four-dimensional volume, and so on.
An $n$-covector must input $n$ vectors and return a scalar.  Further,
every pair of those vectors must satisfy the laws of area.  We call this extension
of the laws of area to collections of $n$ vectors the \emph{laws of volume}\index{laws
of volume}.  The laws of volume together with multi-linearity provide a sufficient
basis to define $n$-covectors\index{$n$-covector}.

\begin{definition}[$n$-covector]
	Let $V$ be a vector space.
	A function $\bm\alpha:\underbrace{V\times \cdots \times V}_{n}
	\to \R$ is called an \emph{$n$-covector} if it is multi-linear
	and obeys the laws
	of volume. 
\end{definition}

With the wedge product, we can create $n$-covectors (and in turn
$n$-forms).  Let $\bm\alpha$ and $\bm \beta$ be covectors and recall
\begin{align*}
	\bm\alpha\wedge \bm\beta (\vec u,\vec v) &= \bm\alpha(\vec u)\bm\beta(\vec v)
	-\bm\beta(\vec u)\bm\alpha(\vec v).
\end{align*}
If $\bm\gamma$ is another covector, we will recursively define the 3-covector
$\bm\alpha \wedge \bm \beta\wedge\bm\gamma$ by
\[
	\bm\alpha \wedge \bm \beta\wedge\bm\gamma(\vec u,\vec v,\vec w)
	=  \bm\alpha(\vec u)\big(\bm\beta\wedge \bm\gamma (\vec v,\vec w)\big)
	- \bm\beta(\vec u)\big(\bm\alpha\wedge \bm\gamma (\vec v,\vec w)\big)
	+ \bm\gamma(\vec u)\big(\bm\alpha\wedge \bm\beta (\vec v,\vec w)\big).
\]
More succinctly, we might write
$
	\bm\alpha \wedge \bm \beta\wedge\bm\gamma
	=  \bm\alpha\cdot \bm\beta\wedge \bm\gamma
	- \bm\beta\cdot \bm\alpha\wedge \bm\gamma
	+ \bm\gamma\cdot \bm\alpha\wedge \bm\beta
$ where ``\,$\cdot$\,'' means multiplication, not a dot product\footnote{
You might recognize this formula as related to the cofactors expansion
of the determinant of a matrix.
}.  Generalizing this, we can define the wedge product of any number
of covectors.

\begin{definition}[Wedge Product]
	Let $\bm\alpha^1,\ldots, \bm\alpha^n$ be covectors.  Then the
	$n$-covector $\bm\alpha^1\wedge\cdots \wedge \bm\alpha^n$ is defined by
	the recursive formula
	\[
		\bm\alpha^1\wedge\cdots \wedge \bm\alpha^n
		=\sum_{i=1}^n (-1)^{i+1} \bm\alpha^i\cdot (\bm\alpha^1\wedge \cdots \wedge 
		\bm\alpha^{i-1}\wedge \bm\alpha^{i+1}\wedge \cdots \wedge \bm\alpha^n).
	\]
\end{definition}
This definition merely says: to find $\bm\alpha^1\wedge\cdots \wedge \bm\alpha^n$,
take each $\bm\alpha^i$ out one by one; multiply $\bm\alpha^i$ by the wedge
product of the remaining terms in order; sum the result together with a $+$
if $i$ is odd and a $-$ if $i$ is even.

It can be cumbersome to evaluate an $n$-covector directly from the definition.
Fortunately, linear algebra provides
efficient ways to evaluate an $n$-fold
wedge product.  We won't go through them here, but we will explore
some consequences of the definition.

First off, one must verify that $\bm\alpha^1\wedge\cdots \wedge \bm\alpha^n$
follows the laws of volume.  A direct consequence is that the wedge
product of any number of covectors is \emph{alternating}\index{alternating}.
That is,
\[
\bm\alpha^1\wedge \cdots \wedge 
		\bm\alpha^{i}\wedge \bm\alpha^{i+1}\wedge \cdots \wedge \bm\alpha^n
=
-\bm\alpha^1\wedge \cdots \wedge
		\bm\alpha^{i+1}\wedge \bm\alpha^{i}\wedge \cdots \wedge \bm\alpha^n.
\]
In other words, switching adjacent pairs in a wedge product negates the product.  
By using the alternating
property of the wedge product, we can rearrange large wedge products.   
The laws of volume state if the expression $\bm\beta\wedge\bm\beta$
ever occurs in a wedge product, the wedge product must be zero.
By using the alternating
property, we conclude that if two $\bm\beta$'s ever occur in a wedge product, the product must be zero\footnote{
Move the $\bm\beta$'s to the back---if there are two or more you'll see a $\bm\beta\wedge\bm\beta$.}.

\subsection{$n$-forms}
We define $n$-forms and integration of $n$-forms\index{$n$-form}
analogously to 1-forms and 2-forms.

\begin{definition}[$n$-form]
	An $n$-form on the space $\R^n$ is a function $\bm\alpha:\R^n
	\to\Set{\text{$n$-covectors for }\R^n}$.
\end{definition}
\begin{definition}[Integration of $n$-forms]
	Let $\mathcal R$ be an $n$-dimensional oriented region
	and let $\bm\alpha$ be an $n$-form.  We define
	\[
		\int_{\mathcal R} \bm\alpha = \int_{a_1}^{b_1}\cdots
		\int_{a_n}^{b_n} \bm\alpha\formarg{
		\vec p(\vec t)}{
			\tfrac{\partial \vec p}{\partial x_1}(\vec t),
			\ldots,
			\tfrac{\partial \vec p}{\partial x_n}(\vec t)
		} \,\d t_n\cdots \d t_1
	\]
	where $\vec p:[a_1,b_1]\times\cdots\times[a_n,b_n]\to\mathcal R$
	is an orientation-preserving parameterization of $\mathcal R$ and
	$\vec t=(t_1,\ldots, t_n)$.
\end{definition}

\subsection{3-forms}

Of particular interest to us are 3-forms.  3-forms measure oriented volume,
and in three dimensions, there's only one ``direction'' a volume can point.
This is reflected in the fact that all 3-forms on $\R^n$ are multiples of
a single element.

\begin{definition}[Standard Basis for 3-forms]
	The standard basis for 3-forms on $\R^3$ is 
	$\dd x\wedge \dd y\wedge \dd z$.
\end{definition}

Because every 3-form on $\R^3$ is
a multiple of $\dd x\wedge \dd y\wedge \dd z$, integrals of 3-forms in $\R^3$
are particularly easy.  In fact, if $\mathcal R\subseteq \R^3$ is an oriented region,
\[
	\int_{\mathcal R} \dd x\wedge \dd y\wedge \dd z
\]
is just the oriented volume of $\mathcal R$.

\section{The Exterior Derivative}

In calculus, we use the symbol ``$\d$'' to notate both the variable of integration
as well as differentiation.  This notation helps us remember the relationship 
between integrals and derivatives---if we integrate a derivative we get back
to (basically) the same thing.  So far, our use of ``$\dd$'' seems to coincide
with traditional integral notation, but it also means something in terms
of derivatives.  Namely, ``$\dd$'' is the symbol for the \emph{exterior derivative}\index{exterior derivative}.

Before we define the exterior derivative, we need one bit of notation.
\begin{definition}
	For a point $\vec x=(x_1,\ldots,x_n)\in \R^n$, vectors
	$\vec v_1,\ldots,\vec v_n\in\R^n$ 
	and scalars $t_1,\ldots, t_n\in \R$, define 
	$B_{\formarg{\vec x}{\vec v_1,\ldots, \vec v_n}}(t_1,\ldots, t_n)$ 
	to be the oriented parallelotope with based at $\vec x$ with
	sides $t_1\vec v_1, t_2\vec v_2, \ldots, t_n\vec v_n$.
\end{definition}

So as not to get bogged down with the intricacies of orientation, let's go
through some examples.  If $\vec x\in \R^1$, then $B_{\formarg{\vec x}{\xhat}}(t)$ is a line
segment of length $t$ oriented left to right if $t>0$ and right to left if $t<0$.
If $\vec x\in \R^2$, then $B_{\formarg{\vec x}{\xhat,\yhat}}(a,b)$ 
is a rectangle with side lengths
$a$ and $b$ oriented upwards (i.e., in a right-handed way) if $a,b>0$.  Similarly,
if $\vec x\in\R^3$, then $B_{\formarg{\vec x}{\xhat,\yhat,\zhat}}(a,b,c)$ 
is a box with side lengths $a$,$b$,
and $c$ oriented in a right-handed way (assuming $a,b,c>0$), whereas 
$B_{\formarg{\vec x}{\yhat,\xhat,\zhat}}(a,b,c)$ is a box with side lengths $a,b,c$
oriented in a left-handed way (assuming $a,b,c>0$).

$B$ should be a familiar object and can be used to tile $n$-dimensional space.
For example, if we use the shorthand $B_{\vec x} = B_{\formarg{\vec x}{\xhat,\yhat}}(1,1)$,
then
$B_{(0,0)}$, $B_{(1,0)}$, $B_{(0,1)}$,
and $B_{(1,1)}$ perfectly fills out a $2\times 2$ square in $\R^2$.

\begin{center}
	\begin{tikzpicture}[>=latex, scale=2]

	\def\mycolor{mypink}

		\coordinate (A) at (0,0);
		\coordinate (B) at (1,0);
		\coordinate (C) at (1,1);
		\coordinate (D) at (0,1);

		\draw[\mycolor,fill opacity=0.05,thick, fill=\mycolor] (A) -- (B)-- (C) -- (D) -- cycle;

		\draw[draw=none] (A) -- (B) node[pos=.3,above] (V1) {};
	\draw[draw=none] (B) -- (C)  node[pos=.3,left] (V2) {}node[near end] (X) {}node[near start] (Y) {};
	\draw[draw=none] (C) -- (D)  node[pos=.3,below] (V3) {};
	\draw[draw=none] (D) -- (A) node[pos=.3,right] (V4) {} ;

	\def\dd{.5}
	\draw (.5,.5) node {$B_{(0,0)}$};

\begin{scope}[shift={(1.02,0)}]
	\def\mycolor{myorange}

		\coordinate (A) at (0,0);
		\coordinate (B) at (1,0);
		\coordinate (C) at (1,1);
		\coordinate (D) at (0,1);

		\draw[\mycolor,fill opacity=0.05,thick, fill=\mycolor] (A) -- (B)-- (C) -- (D) -- cycle;

		\draw[draw=none] (A) -- (B) node[pos=.3,above] (V1) {};
	\draw[draw=none] (B) -- (C)  node[pos=.3,left] (V2) {}node[near end] (X) {}node[near start] (Y) {};
	\draw[draw=none] (C) -- (D)  node[pos=.3,below] (V3) {};
	\draw[draw=none] (D) -- (A) node[pos=.3,right] (V4) {} ;

	\def\dd{.5}
	\draw (.5,.5) node {$B_{(1,0)}$};

\end{scope}
\begin{scope}[shift={(1.02,1.02)}]
	\def\mycolor{mypink}

		\coordinate (A) at (0,0);
		\coordinate (B) at (1,0);
		\coordinate (C) at (1,1);
		\coordinate (D) at (0,1);

		\draw[\mycolor,fill opacity=0.05,thick, fill=\mycolor] (A) -- (B)-- (C) -- (D) -- cycle;

		\draw[draw=none] (A) -- (B) node[pos=.3,above] (V1) {};
	\draw[draw=none] (B) -- (C)  node[pos=.3,left] (V2) {}node[near end] (X) {}node[near start] (Y) {};
	\draw[draw=none] (C) -- (D)  node[pos=.3,below] (V3) {};
	\draw[draw=none] (D) -- (A) node[pos=.3,right] (V4) {} ;

	\def\dd{.5}
	\draw (.5,.5) node {$B_{(1,1)}$};
\end{scope}
\begin{scope}[shift={(0,1.02)}]
	\def\mycolor{myorange}

		\coordinate (A) at (0,0);
		\coordinate (B) at (1,0);
		\coordinate (C) at (1,1);
		\coordinate (D) at (0,1);

		\draw[\mycolor,fill opacity=0.05,thick, fill=\mycolor] (A) -- (B)-- (C) -- (D) -- cycle;

		\draw[draw=none] (A) -- (B) node[pos=.3,above] (V1) {};
	\draw[draw=none] (B) -- (C)  node[pos=.3,left] (V2) {}node[near end] (X) {}node[near start] (Y) {};
	\draw[draw=none] (C) -- (D)  node[pos=.3,below] (V3) {};
	\draw[draw=none] (D) -- (A) node[pos=.3,right] (V4) {} ;

	\def\dd{.5}
	\draw (.5,.5) node {$B_{(0,1)}$};
\end{scope}

	\end{tikzpicture}
\end{center}

The fact that $B$ is oriented is also very important.  If $B$ is
oriented, then $\partial B$, the boundary of $B$, is also oriented.
If we consider the partition of 
$B_{\formarg{\vec 0}{\xhat,\yhat}}(2,2)=[0,2]\times[0,2]$ by 
$B_{(0,0)}$, $B_{(1,0)}$, $B_{(0,1)}$,
and $B_{(1,1)}$, we see that
\[
	B_{\formarg{\vec 0}{\xhat,\yhat}}(2,2)
	=B_{(0,0)}\cup B_{(1,0)}\cup B_{(0,1)}\cup B_{(1,1)}
\]
and that
\[
	\partial B_{\formarg{\vec 0}{\xhat,\yhat}}(2,2)=\partial B_{(0,0)}\cup \partial B_{(1,0)}\cup \partial B_{(0,1)}\cup \partial B_{(1,1)},
\]
provided we interpret the union of two overlapping and oppositely-oriented sets as empty.

\begin{center}
	\begin{tikzpicture}[>=latex, scale=2]
	\def\mycolor{mypink}

		\coordinate (A) at (0,0);
		\coordinate (B) at (1,0);
		\coordinate (C) at (1,1);
		\coordinate (D) at (0,1);

		\draw[\mycolor,fill opacity=0.05,thick] (A) -- (B)-- (C) -- (D) -- cycle;

		\draw[draw=none] (A) -- (B) node[pos=.3,above] (V1) {};
	\draw[draw=none] (B) -- (C)  node[pos=.3,left] (V2) {}node[near end] (X) {}node[near start] (Y) {};
	\draw[draw=none] (C) -- (D)  node[pos=.3,below] (V3) {};
	\draw[draw=none] (D) -- (A) node[pos=.3,right] (V4) {} ;

	\def\dd{.5}
	\draw[->, \mycolor, thick] (V1) -- +(\dd, 0);
	\draw[->, \mycolor, thick] (V2) -- +(0, \dd);
	\draw[->, \mycolor, thick] (V3) -- +(-\dd, 0);
	\draw[->, \mycolor, thick] (V4) -- +(0, -\dd);
	\draw (.5,.5) node {$\partial B_{(0,0)}$};

\begin{scope}[shift={(1.02,0)}]
	\def\mycolor{myorange}

		\coordinate (A) at (0,0);
		\coordinate (B) at (1,0);
		\coordinate (C) at (1,1);
		\coordinate (D) at (0,1);

		\draw[\mycolor,fill opacity=0.05,thick] (A) -- (B)-- (C) -- (D) -- cycle;

		\draw[draw=none] (A) -- (B) node[pos=.3,above] (V1) {};
	\draw[draw=none] (B) -- (C)  node[pos=.3,left] (V2) {}node[near end] (X) {}node[near start] (Y) {};
	\draw[draw=none] (C) -- (D)  node[pos=.3,below] (V3) {};
	\draw[draw=none] (D) -- (A) node[pos=.3,right] (V4) {} ;

	\def\dd{.5}
	\draw[->, \mycolor, thick] (V1) -- +(\dd, 0);
	\draw[->, \mycolor, thick] (V2) -- +(0, \dd);
	\draw[->, \mycolor, thick] (V3) -- +(-\dd, 0);
	\draw[->, \mycolor, thick] (V4) -- +(0, -\dd);
	\draw (.5,.5) node {$\partial B_{(1,0)}$};

\end{scope}
\begin{scope}[shift={(1.02,1.02)}]
	\def\mycolor{mypink}

		\coordinate (A) at (0,0);
		\coordinate (B) at (1,0);
		\coordinate (C) at (1,1);
		\coordinate (D) at (0,1);

		\draw[\mycolor,fill opacity=0.05,thick] (A) -- (B)-- (C) -- (D) -- cycle;

		\draw[draw=none] (A) -- (B) node[pos=.3,above] (V1) {};
	\draw[draw=none] (B) -- (C)  node[pos=.3,left] (V2) {}node[near end] (X) {}node[near start] (Y) {};
	\draw[draw=none] (C) -- (D)  node[pos=.3,below] (V3) {};
	\draw[draw=none] (D) -- (A) node[pos=.3,right] (V4) {} ;

	\def\dd{.5}
	\draw[->, \mycolor, thick] (V1) -- +(\dd, 0);
	\draw[->, \mycolor, thick] (V2) -- +(0, \dd);
	\draw[->, \mycolor, thick] (V3) -- +(-\dd, 0);
	\draw[->, \mycolor, thick] (V4) -- +(0, -\dd);
	\draw (.5,.5) node {$\partial B_{(1,1)}$};
\end{scope}
\begin{scope}[shift={(0,1.02)}]
	\def\mycolor{myorange}

		\coordinate (A) at (0,0);
		\coordinate (B) at (1,0);
		\coordinate (C) at (1,1);
		\coordinate (D) at (0,1);

		\draw[\mycolor,fill opacity=0.05,thick] (A) -- (B)-- (C) -- (D) -- cycle;

		\draw[draw=none] (A) -- (B) node[pos=.3,above] (V1) {};
	\draw[draw=none] (B) -- (C)  node[pos=.3,left] (V2) {}node[near end] (X) {}node[near start] (Y) {};
	\draw[draw=none] (C) -- (D)  node[pos=.3,below] (V3) {};
	\draw[draw=none] (D) -- (A) node[pos=.3,right] (V4) {} ;

	\def\dd{.5}
	\draw[->, \mycolor, thick] (V1) -- +(\dd, 0);
	\draw[->, \mycolor, thick] (V2) -- +(0, \dd);
	\draw[->, \mycolor, thick] (V3) -- +(-\dd, 0);
	\draw[->, \mycolor, thick] (V4) -- +(0, -\dd);
	\draw (.5,.5) node {$\partial B_{(0,1)}$};
\end{scope}

\draw[->, blue, thick] (.25, -.1) -- (1.75, -.1);
\draw[<-, blue, thick, shift={(0,2)}] (.25, .1) -- (1.75, .1);
\draw[->, blue, thick, shift={(2,0)}] ( .1, .25) -- (.1, 1.75);
\draw[<-, blue, thick, shift={(0,0)}] ( -.1, .25) -- (-.1, 1.75);

	\end{tikzpicture}
\end{center}

This picture should is just like the one we used when deriving the divergence theorem
and Stokes's theorem.


Now, we can define the \emph{exterior derivative}\index{exterior derivative}.
\begin{definition}[Exterior Derivative]
	The \emph{exterior derivative}, notated $\dd$, is a map that
	takes $n$-forms to $(n+1)$-forms.  If $\bm\alpha$ is an $n$-form,
	then
	\[
		\dd \bm\alpha\formarg{\vec x}{\vec v_1,\ldots,\vec v_{n+1}}
		=\lim_{t_1,\ldots,t_{n+1}\to 0}
		\frac{1}{t_1t_2\cdots t_{n+1}}\int_{\partial B} \bm \alpha
	\]
	where $B = B_{\formarg{\vec x}{\vec v_1,\ldots,\vec v_{n+1}}}(t_1,\ldots, t_{n+1})$.
\end{definition}

A quick sanity check shows that since $B = B_{\formarg{\vec x}{\vec v_1,\ldots,\vec v_{n+1}}}(t_1,\ldots, t_{n+1})$
is an $n+1$ dimensional object, $\partial B$ is an $n$-dimensional object over which an $n$-form $\bm\alpha$ could be
integrated.  One must still prove that the result of $\dd\bm\alpha$ is an $(n+1)$-form
in that $\dd \bm\alpha(\vec x)$ outputs an honest $(n+1)$-covector (multi-linear
and obeying the laws of volume).  Showing this relies on smoothness of $\bm\alpha$
as an $n$-form, which is beyond the scope of this book\footnote{
Though thinking in high-dimensions and covectors can be hard, the
idea is familiar---at every point there needs to exist a co-tangent plane!}.

\subsection{The Exterior Derivative on 0-forms}

We haven't yet defined 0-forms, but we've already been using them.  Since
an $n$-covector inputs $n$ vectors and outputs a scalar, by extension,
a $0$-covector must input zero vectors and return a scalar.  A 0-form\index{0-form}
must input a point and output a 0-covector, which can be represented by
a scalar.  Therefore, we can think of scalar fields as 0-forms.

Now, 0-forms are to be integrated over oriented zero dimensional objects.
These are just points with a positive or negative orientation.  There's really not
much choice when defining the integral over a point.  Thus, if $\bm\alpha$
is a 0-form, 
\[
	\int_{\vec x}\bm\alpha = \bm\alpha(\vec x)
\]
if $\vec x$ is oriented positively and 
\[
	\int_{\vec x}\bm\alpha = -\bm\alpha(\vec x)
\]
if $\vec x$ is oriented negatively\footnote{
I hear you complaining---something doesn't smell right!  $\bm\alpha(\vec x)$
should be a 0-covector which you can identify with a scalar but isn't
truly a scalar.  Further, we never had to add ``$-$'' signs to the 
\emph{outside} of the integral for other $n$-forms.  What gives?

We'll, it turns out, a $0$-covector should really be a function
that inputs no vectors \emph{and} and orientation, and returns a scalar.  This is 
true for $n$-covectors in general.  However, for
$n>0$, we could always deduce orientation from how we ordered the input-vectors.  No
input vectors means we have to specify the orientation manually.  If it
makes you feel better, write $\bm\alpha\formarg{\vec x}{+}$ and $\bm\alpha\formarg{\vec x}{-}$
when using $0$-forms.
}.  Further, integrals should be additive
over disjoint domains, so
\[
	\int_{\Set{\vec x_1,\ldots,\vec x_n}} \bm\alpha = \bm\alpha(\vec x_1)
	+ \cdots+\bm\alpha(\vec x_n).
\]

Finally, we're ready for action.  Let $\bm\alpha$ be a 0-form, and let's compute
$\dd \bm\alpha$.  By definition
\[
	\dd \bm\alpha\formarg{\vec x}{\vec v}
	= \lim_{t\to 0} \frac{1}{t} \int_{\partial B} \bm\alpha
\]
where $B=B_{\formarg{\vec x}{\vec v}}(t)$.  But, $B$ is just a line segment
and so $\partial B$ consist of two points: $\vec x+t\vec v$ oriented positively,
and $\vec x$ oriented negatively.

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
			name=plot0,
		    anchor=origin,
		    set layers=standard,
		    disabledatascaling,
		    xmin=0,xmax=5,
		    ymin=-0,ymax=3,
		    x=1cm,y=1cm,
		    grid=both,
		    grid style={line width=.1pt, draw=gray!10},
		    %major grid style={line width=.2pt,draw=gray!50},
		    axis lines=middle,
		    minor tick num=0,
		    enlargelimits={abs=0.2},
		    axis line style={latex-latex},
		    ticklabel style={font=\tiny,fill=white},
		    xlabel style={at={(ticklabel* cs:1)},anchor=north west},
		    ylabel style={at={(ticklabel* cs:1)},anchor=south west}
		]



		\draw[->, thick, myorange, densely dashed] (1,1) -- (4,2) node[right] {$\vec v$} node[pos=.75] (A){};
		\draw[thick, mypink] (1,1) -- ($.75*(3,1)+(1,1)$) node[midway, below right] {$B$};
		\draw[fill=black, draw=none] (1,1) circle (1.5pt) node [below] {$\vec x$};
		\draw[fill=black, draw=none] (A) circle (1.5pt) node [above left] {$\vec x+t\vec v$};		
		\end{axis}

		\begin{axis}[
			at=(plot0.right of south east), xshift=.5cm,
		    xmin=0,xmax=5,
		    ymin=-0,ymax=3,
		    x=1cm,y=1cm,
		    grid=both,
		    grid style={line width=.1pt, draw=gray!10},
		    %major grid style={line width=.2pt,draw=gray!50},
		    axis lines=middle,
		    minor tick num=0,
		    enlargelimits={abs=0.2},
		    axis line style={latex-latex},
		    ticklabel style={font=\tiny,fill=white},
		    xlabel style={at={(ticklabel* cs:1)},anchor=north west},
		    ylabel style={at={(ticklabel* cs:1)},anchor=south west}
		]



		\draw[->, thick, myorange, densely dashed, draw=none] (1,1) -- (4,2) node[pos=.75] (A){};
		\draw[thick, mypink, draw=none] (1,1) -- ($.75*(3,1)+(1,1)$) node[midway] {$\partial B$};
		\draw[fill=mypink, draw=none] (1,1) circle (1.5pt) node(X) {};
		\draw[fill=mypink, draw=none] (A) circle (1.5pt) node  (Y) {};		
		\end{axis}
	\draw (X) node [below right, xshift=-.25cm,yshift=-.1cm]  {$-$ orientation} (Y) node[above right, xshift=-.25cm,yshift=.1cm] {$+$ orientation};
	\end{tikzpicture}
\end{center}

Thus,
\[
	\dd \bm\alpha\formarg{\vec x}{\vec v}
	= \lim_{t\to 0} \frac{\bm\alpha(\vec x+t\vec v) - \bm\alpha(\vec x)}{t},
\]
which is just the definition of the directional derivative of $\bm\alpha$ at $\vec x$
in the direction $\vec v$.  Further, $\dd \bm\alpha$ is a 1-form, which
means to each point it assigns a covector.  We know that if $\bm\beta$ is a covector,
$\bm\beta(\vec v) = \vec \beta \cdot \vec v$ for some vector $\vec \beta$.
But, if we recall, the gradient of a function was exactly defined to be the
vector that when dot producted with a direction gave the directional derivative.
Thus,
\[
	\dd \bm\alpha\formarg{\vec x}{\vec v} = \nabla \bm\alpha(\vec x)\cdot \vec v,
\]
and we'd be justified in writing ``$\dd \bm\alpha(\vec x) \approx \nabla \bm\alpha(\vec x)$.''

\subsection{The Exterior Derivative on 1-forms}
We're on a roll with 0-forms.  The exterior derivative replaces the need
for the gradient operator.  What does it do for 1-forms?  Well,
let $\bm\alpha$ be a 1-form.  Then, $\dd \bm\alpha$ inputs a point and outputs
a 2-covector and is defined by 
\[
	\dd \bm\alpha\formarg{\vec x}{\vec u,\vec v}
	= \lim_{\Delta a,\Delta b\to 0} \frac{1}{\Delta a\Delta b} \int_{\partial B} \bm\alpha
\]
where $B=B_{\formarg{\vec x}{\vec u,\vec v}}(\Delta a,\Delta b)$. We're using $\Delta a$
and $\Delta b$ on purpose because $\partial B$ is the oriented perimeter of a parallelogram.

\begin{center}
	\begin{tikzpicture}[>=latex, scale=2]
	\def\mycolor{myorange}

		\coordinate (A) at (0,0);
		\coordinate (B) at (1,0);
		\coordinate (C) at (2,1);
		\coordinate (D) at (1,1);

		\draw[\mycolor,fill opacity=0.05,thick, fill] (A) -- (B)-- (C) -- (D) -- cycle;
		
		\draw[mypink, thick, ->, densely dashed] (A) -- (B) node[midway, below] {$\vec u$};
		\draw[mypink, thick, ->, densely dashed] (A) -- (D) node[midway, above left] {$\vec v$};

	\draw (1,.5) node {$B$};


\begin{scope}[shift={(2,0)}]
		\coordinate (A) at (0,0);
		\coordinate (B) at (1,0);
		\coordinate (C) at (2,1);
		\coordinate (D) at (1,1);

		\draw[\mycolor,fill opacity=0.05,thick] (A) -- (B)-- (C) -- (D) -- cycle;

		\draw[draw=none] (A) -- (B) node[pos=.3,above] (V1) {};
	\draw[draw=none] (B) -- (C)  node[pos=.2,left, xshift=-.05cm] (V2) {}node[near end] (X) {}node[near start] (Y) {};
	\draw[draw=none] (C) -- (D)  node[pos=.3,below] (V3) {};
	\draw[draw=none] (D) -- (A) node[pos=.2,right, xshift=.05cm] (V4) {} ;

	\def\dd{.5}
	\draw[->, \mycolor, thick] (V1) -- +(\dd, 0);
	\draw[->, \mycolor, thick] (V2) -- +({\dd}, \dd);
	\draw[->, \mycolor, thick] (V3) -- +(-\dd, 0);
	\draw[->, \mycolor, thick] (V4) -- +(-\dd, -\dd);
	\draw (1,.5) node {$\partial B$};
\end{scope}


	\end{tikzpicture}
\end{center}

This limit looks exactly like the limit used to compute curl.  In fact,
if we refer back to the definition of the $\vec w$-component of the curl
of a vector field, the definitions align flawlessly.  Let $\vec F=(f_x,f_y,f_z)$
and consider the corresponding 1-form $\bm\alpha = f_x\dd x+f_y\dd y+f_z\dd z$.
Then,
\begin{align*}
	\Curl_{\xhat} \vec F(\vec x) &= \dd \bm\alpha\formarg{\vec x}{\yhat,\zhat}\\
	\Curl_{\yhat} \vec F(\vec x) &= \dd \bm\alpha\formarg{\vec x}{\zhat,\xhat}\\
	\Curl_{\zhat} \vec F(\vec x) &= \dd \bm\alpha\formarg{\vec x}{\xhat,\yhat}.
\end{align*}
Note the order we had to list $\xhat$, $\yhat$, $\zhat$ in to ensure $\dd\bm\alpha$
matched the curl.  We've seen this order before---in the standard basis for 2-forms
on $\R^3$.  Since we know we can find the curl of a vector field on $\R^2$ in
any direction using just the $\xhat$, $\yhat$, and $\zhat$ components, we'd be
justified in writing ``$\dd \bm\alpha(\vec x) \approx \nabla \times \bm\alpha(\vec x)$''
when $\bm\alpha$ is a 1-form on three-dimensional space\footnote{
Can you begin to see why curl could not be a ``vector'' in $\R^4$?  The standard basis
for 2-forms in $\R^4$ has six elements.  Therefore, you would need to know the
curl in six directions to describe the component of curl in an arbitrary direction.
}.

\subsection{The Exterior Derivative on 2-forms}
Can this luck streak keep going?  Let $\bm\alpha$ is a 2-form.  We know
\[
	\dd \bm\alpha\formarg{\vec x}{\vec u,\vec v,\vec w}
	= \lim_{\Delta a,\Delta b,\Delta c\to 0} \frac{1}{\Delta a\Delta b\Delta c} \int_{\partial B} \bm\alpha
\]
where $B=B_{\formarg{\vec x}{\vec u,\vec v,\vec w}}(\Delta a,\Delta b,\Delta c)$.
Sure enough, this is exactly the limit of the flux through a little box as the size of the
box goes to zero.  After some careful analysis, if $\bm\alpha$ is a 2-form on $\R^3$,
we might write ``$\dd \bm\alpha(\vec x) \approx \nabla \cdot \bm\alpha(\vec x)$,''
and so the exterior derivative captures divergence, curl, and gradient in a single
definition.

\subsection{Practical Computing with $\dd$}

We've seen that $\dd$ can replace gradient, curl, and divergence in $\R^3$
and without changing any definitions, we can also bring these concepts to $\R^n$.
But, using limits to compute derivatives can be cumbersome---especially when
we know derivative rules from single-variable calculus.

The exterior derivative comes with its own set of rules.

\begin{definition}[Exterior Derivative Rules]
	The exterior derivative, $\dd$, obeys the following rules.
	\begin{enumerate}
		\item If $f(a,b,c,\ldots)$ is a 0-form, then
		\[
			\dd f = \tfrac{\partial f}{\partial a} \dd a+
			\tfrac{\partial f}{\partial b} \dd b + 
			\tfrac{\partial f}{\partial c} \dd c + \cdots.
		\]
		\item If $\bm\alpha$ is an $n$-form, then
		\[
			\dd(\dd \bm\alpha) = 0.
		\]
		\item If $\bm\alpha$ is an $n$-form and $\bm\beta$
		is an $m$-form, then $\dd$ obeys the following product
		rule:
		\[
			\dd(\bm\alpha\wedge \bm\beta)= 
			(\dd \bm\alpha)\wedge \bm\beta + (-1)^n \bm\alpha\wedge (\dd \bm\beta).
		\]
	\end{enumerate}
\end{definition}

The exterior derivative rules could actually be taken as axioms---the exterior
derivative is the \emph{only} non-trivial operator that satisfies all
the rules\footnote{
Many textbooks forgo limits and define the exterior derivative axiomatically.
This makes the exterior derivative simpler in some respects, but it also
makes it harder to see how it connects to physical concepts like work
and curl.}.  With the exterior derivative rules in hand, we're ready for action.

\begin{example}
	Compute $\dd f$ where $f(x,y,z) = xy-z$ is a 0-form.

	Since $f$ is a 0-form, we more-or-less mimic the chain rule.
	\[
		\dd f = \dd (xy-z) = \dd (xy) -\dd z = y\dd x+x\dd y-\dd z.
	\]
\end{example}
\begin{example}
	Compute $\dd \bm f$ where $\bm f(x,y,z) = -y\dd x+x\dd y +\dd z$ is a 1-form.

	Applying the product rule for the exterior derivative, we have
	\begin{align*}
		\dd \bm f &= \dd(-y\dd x+x\dd y +\dd z) \\
		&= -\dd y\wedge \dd x - y\dd(\dd x) + \dd x\wedge \dd y + x\dd(\dd y) + \dd(\dd z)\\
		&= -\dd y\wedge \dd x+\dd x\wedge \dd y\\
		&= 2\dd x\wedge \dd y.
	\end{align*}
\end{example}
\begin{example}
	Compute $\dd \bm g$ where $\bm g(x,y,z) = 2yx\dd y\wedge \dd z
	+ 4\dd z\wedge \dd x+z^2\dd x\wedge \dd y$ is a 2-form.

	Applying the product rule for the exterior derivative, we have
	\begin{align*}
		\dd \bm g &= \dd(2yx\dd y\wedge \dd z) \\
		&= \dd(2xy)\wedge \dd y\wedge \dd z + 2xy\dd(\dd y)\wedge \dd z -2xy\dd y\wedge \dd(\dd z)\\
		&= \dd(2xy)\wedge \dd y\wedge \dd z\\
		&= 2(y\dd x+x\dd y)\wedge \dd y\wedge \dd z\\
		&= 2y\dd x\wedge \dd y\wedge \dd z,
	\end{align*}
	with the last equality following because $\dd y\wedge \dd y=0$.
\end{example}

\subsection{Changing Coordinates}

Differential forms make doing integrals in other coordinate systems
almost trivial---it computes the volume form automatically.

Polar coordinates are related to rectangular coordinates by the equations
\[
	x=r\cos\theta \qquad\text{and}\qquad y=r\sin\theta.
\]
Suppose $\mathcal S\subset \R^2$ is a region in the $xy$-plane.  If we wish to find
the area of $\mathcal S$, we need only to compute
\[
	\int_{\mathcal S} \dd x\wedge \dd y.
\]
However, using the exterior derivative, we see
\begin{alignat*}{2}
	\dd x &= \dd (r\cos\theta) &&= \cos\theta \dd r - r\sin\theta \dd \theta\\
	\dd y &= \dd (r\sin\theta) &&= \sin\theta \dd r + r\cos\theta \dd \theta.
\end{alignat*}
Therefore, 
\begin{align*}
	\dd x\wedge \dd y &= (\cos\theta \dd r - r\sin\theta \dd \theta)\wedge 
		(\sin\theta \dd r + r\cos\theta \dd \theta)\\
	&= \cos\theta\sin\theta \dd r\wedge \dd r + r\cos^2\theta \dd r\wedge \dd \theta
	-r\sin^2\theta \dd \theta \wedge \dd r + r^2\cos\theta\sin\theta \dd \theta\wedge \dd \theta\\
	&=r\cos^2\theta \dd r\wedge \dd \theta-r\sin^2\theta \dd \theta \wedge \dd r\\
	&= r\cos^2\theta \dd r\wedge \dd \theta + r\sin^2\theta \dd r \wedge \dd \theta\\
	&= r\dd r\wedge \dd \theta,
\end{align*}
because $\dd r\wedge \dd r$ and $\dd\theta\wedge \dd\theta$ are 0 and $\dd r\wedge 
\dd\theta = -\dd \theta \wedge \dd r$.  We conclude that
\[
	\int_{\mathcal S} \dd x\wedge \dd y = \int_{\mathcal S} r\dd r\wedge \dd \theta,
\]
and have just used differential forms to find the volume form for polar 
coordinates\footnote{
Notice that the terms that got thrown away because $\dd r\wedge \dd r=\dd\theta\wedge \dd\theta=0$
were very similar to the terms that go thrown away from the pre-volume form when
we used a limit process.
}.  Further, if we instead wanted to integrate a function $f:\mathcal S\to \R$
over $\mathcal S$, 
we could do the exact same thing since
\[
	\int_{\mathcal S} f\dd x\wedge \dd y = \int_{\mathcal S} fr\dd r\wedge \dd \theta.
\]
As long as we can determine what the value of $f$ is at a point in polar coordinates, 
we can compute the integral with ease.

\begin{example}
	Let $\mathcal S$ be the unit disk centered at $\vec 0$ and let $f(x,y) = x^2+2y^2$.
	Compute $\int_{\mathcal S} f\dd x\wedge \dd y$.

	We can express $f$ in polar coordinates.
	\[
		f(x,y) = x^2+2y^2=(x^2+y^2)+y^2 = r^2+r^2\sin^2 \theta
	\]
	Therefore
	\begin{align*}
		\int_{\mathcal S} f\dd x\wedge \dd y
		&=\int_{\mathcal S} r^2+r^2\sin^2 \theta\dd x\wedge \dd y\\
		&=\int_{\mathcal S} r^2+r^2\sin^2 \theta\dd x\wedge \dd y\\
		&=\int_{\mathcal S} (r^2+r^2\sin^2 \theta) r\dd r\wedge \dd \theta\\
		&=\int_{\mathcal S} r^3+r^3\sin^2 \theta \dd r\wedge \dd \theta\\
		&= \int_{\theta=0}^{\theta =2\pi} \int_{r=0}^{r=1} r^3+r^3\sin \theta \,\d r\d \theta = \frac{3\pi}{4}.
	\end{align*}
\end{example}

\section{The Hodge Dual}

There is a certain symmetry to $n$-covectors and $n$-forms.  We haven't delved
deeply into the linear algebra reasoning behind it, but (at least for $\R^3$)
the standard basis for $n$-covectors and $n$-forms always consisted of a linear
combinations of the wedge product of $n$ covectors/1-forms.  This is true in
general.

\begin{theorem}
	\label{THMSTDBASIS}
	Let $B^1=\Set{\bm e^1,\ldots, \bm e^n}$ be the standard basis of covectors
	for $\R^n$ and let \[B^k=\Set{\bm\alpha\given \bm\alpha\text{ is the wedge
	product of $k$ items from }B_1}.\]  Then, every $k$-covector can be expressed
	as a linear combination of $k$-covectors from $B^k$.
\end{theorem}
Theorem \ref{THMSTDBASIS} applies equally well to $k$-forms and has an immediate consequence.

For the time being, assume we are always taking about $k$-covectors on $\R^n$.
Since $\bm \alpha\wedge \bm\alpha=0$ if $\bm\alpha$ is a covector\footnote{
Remember, this may not be true for $n$-covectors with $n>1$, and certainly isn't true
for $0$-covectors.
}, every non-zero element in $B^k$ must be a wedge product of $k$ \emph{different}
covectors from $B^1$.  Further, if $\bm\alpha,\bm\beta\in B^k$ are wedge products of
the same $k$ covectors from $B^1$, by applying the alternating
property, we know either $\bm\alpha=\bm\beta$ or $\bm\alpha=-\bm\beta$, and so
$\bm\beta$ is a linear combination of $\bm\alpha$.   This means, to create all
$k$-covectors as linear combinations, all we really need a set consisting of one wedge
product for each distinct set $k$ covectors from $B^1$.

Given $n$ items, $\binom{n}{k}=\frac{n!}{k!(n-k)!}$ is the number of ways to choose\index{choose}\index{binomial coefficient}
$k$ things from a set of $n$ things with no repeats provided $0\leq k\leq n$.  The
number $\binom{n}{k}$ is called a \emph{binomial coefficient}\footnote{
Note that since $0!=1$, we have $\binom{n}{0} = 1$, like it should.  There's only
one way to choose nothing.
}.  If $k<0$ or $k>n$, there are no ways to pick $k$ items from a set
of size $n$ without repeats.  We will extend notation to say that $\binom{n}{k}=0$ if
$k<0$ or $k>n$.

This alone is significant.  Our argument now says that if $k>n$, every
$k$-covector can be represented as a linear combination of $k$-covectors
from a set of size $\binom{n}{k}=0$.  In other words, every $k$-covector for $k>n$
is trivial.

\begin{definition}[Basis \& Dimension]
	Let $V$ be a vector space.
	A set $B\subseteq V$ is called a \emph{basis} for $V$
	if (i) every element of $V$ can be represented as a linear combination
	of elements in $B$ and (ii) if any element is removed from $B$, property (i)
	fails.

	The \emph{dimension} of $V$, notated $\Dim(V)$, is the size of a basis for
	$V$.
\end{definition}
You can think of a basis for $V$ as a minimal set of vectors for which every element
in $V$ can be written as a linear combination.  Every element in a basis is essential,
since removing any element means you can no longer write everything in $V$ as a 
linear combination.

Our argument about the number of essential elements in $B^k$ along with the language
of dimension allows us to state the following theorem.

\begin{theorem}
	\label{THMDIMOFCOK}
	Let $\Lambda^k\R^n$ be the set of all $k$-covectors on $\R^n$.
	The dimension of $\Lambda^k\R^n$ is $\binom{n}{k}$.
\end{theorem}
Again, Theorem \ref{THMDIMOFCOK} applies equally well to $k$-forms.

Let's be concrete for a moment and consider $k$-forms on $\R^3$.  Theorem \ref{THMDIMOFCOK}
says that the dimension of the set 0-forms should be $1$, the dimension of the
set of 1-forms should be $3$, the dimension of the set of 2-forms should be $3$,
the dimension of the set of 3-forms should be $1$, and the dimension of the set
of all higher forms should be $0$.  This is reflected in the standard bases we've 
already introduced.

\begin{center}
	\begin{tabular}{c|c|c}
	&Standard Basis & Dimension\\
	\hline
	0-forms & the function $f(x,y,z)=1$ & $1$\\
	1-forms & $\dd x$, $\dd y$, $\dd z$ & 3\\
	2-forms & $\dd x\wedge \dd y$, $\dd y\wedge \dd z$, $\dd z\wedge \dd x$ & 3\\
	3-forms & $\dd x\wedge \dd y\wedge \dd z$ & 1
	\end{tabular}
\end{center}

Now we can directly see the symmetry.  The dimension of the set
of $k$-forms is $\binom{n}{k}$ which is equal to the dimension of the
set of $(n-k)$-forms\footnote{
	$\binom{n}{k} = \frac{n!}{k!(n-k)!} - \frac{n!}{(n-k)!k!} = \binom{n}{n-k}$
}. In particular, the dimension of the set of 0-covectors is $1$ and the dimension
of the set of $n$-covectors is $1$.

\subsection{The Hodge Star Operator}

A $k$-covector (or $k$-form) on $\R^n$ is a linear combination of the
wedge product of $k$ covectors.  An $n$-covector (or $n$-form) is a linear
combination of the wedge product of $n$ covectors.  Since we have a basis
of $n$ covectors, every $n$-covector is a multiple of this element.  Namely,
every $n$-covector on $\R^n$ is a multiple of $\bm e^1\wedge \cdots \wedge \bm e^n$
and every $k$-covector could be thought as ``missing'' some items in its wedge
product, to top it up to be an $n$-covector.  The \emph{Hodge star operator}\index{Hodge star}\index{Hodge dual}
sends a $k$-covector to its missing half.

\begin{definition}[Hodge Star Operator]
	Let $\Lambda^k\R^n$ be the set of $k$-covectors on $\R^n$.
	The \emph{Hodge star operator} (or \emph{Hodge dual operator}),
	denoted ``$\star$'' is defined on $\Lambda^k\R^n$ as follows.
	Let $\bm\alpha\in \Lambda^k\R^n$ be represented as
	$\bm\alpha = \sum t_i \bm b^i$ where $\bm b^i$ are standard basis
	elements for $\Lambda^k\R^n$.  Then $\star \bm\alpha$ is defined
	to be the $(n-k)$-covector so that
	\[
		\bm\alpha\wedge \star\bm\alpha = \left(\sum t_i^2\right)\bm\omega
	\]
	where $\bm\omega = \bm e^1\wedge \cdots\wedge \bm e^n$.  We call $\star\bm\alpha$
	the \emph{Hodge dual} of $\bm\alpha$.
\end{definition}

The Hodge dual of a $k$-covector $\bm\alpha$ is a kind of compliment that fills
it out to be a full $n$-covector.  But why the $\sum t_i^2$ terms?  We'll, that should
make you think of norms!

Recall that if a vector $\vec v = \sum t_i \vec e_i$ where $\vec e_i$ are the standard
basis vectors for $\R^n$, then
\[
	\vec v\cdot \vec v = \norm{\vec v}^2 = \sum t_i^2.
\]
In the same way, if $\bm\alpha$ is a $k$-covector, we can think of $\bm\alpha\wedge \star
\bm\alpha$ as $\norm{\bm\alpha}^2$.  If we spent some time to make this rigorous,
we would see the same $\sum t_i^2$ for $\norm{\bm\alpha}^2$  as we do for $\norm{\vec v}^2$.

We will now start working with differential forms.
\begin{example}
	Compute $\star \dd x$ and $\star (\dd y\wedge \dd z)$ on $\R^3$.

	Since $\dd x$ is a 1-form on $\R^3$, we know $\star \dd x$ must be a
	2-form.  Further, $\dd x=1\dd x$ is already written in terms of the standard
	basis.  Therefore, $\star \dd x$ satisfies
	\[
		\dd x\wedge \star \dd x = \dd x\wedge \dd y\wedge \dd z,
	\]
	and so $\star \dd x = \dd y\wedge \dd z$.

	Similarly, $\star (\dd y\wedge \dd z)$ is a 1-form so that
	\[
		(\dd y\wedge \dd z)\wedge \star (\dd y\wedge \dd z) =\dd x\wedge \dd y\wedge \dd z.
	\]
	We conclude that $\star (\dd y\wedge \dd z) = \dd x$ since
	\[
		\dd y\wedge \dd z\wedge \dd x = -\dd y\wedge \dd x\wedge \dd z = \dd x\wedge \dd y\wedge \dd z.
	\]
\end{example}
As in the example, it turns out applying ``$\star$'' twice always brings you back to the
same form.

\begin{theorem}
	If $\bm\alpha$ is a $k$-form, then $\star \star \bm\alpha = \bm\alpha$.
\end{theorem}
This is the reason we call ``$\star$'' a \emph{dual} operator\footnote{
Covectors are also called dual vectors for a similar reason.
}.

Since $\R^2$ and $\R^3$ are so important, we will make a table of basis forms and
their Hodge duals.
\begin{center}
	\begin{tabular}{c|c}
	Form & Hodge Dual ($\R^2$)\\
	\hline
	$1$ & $\dd x\wedge \dd y$\\
	$\dd x$ & $\dd y$\\
	$\dd y$ & $-\dd x$
	\end{tabular}
\end{center}
\begin{center}
	\begin{tabular}{c|c}
	Form & Hodge Dual ($\R^3$)\\
	\hline
	$1$ & $\dd x\wedge \dd y\wedge \dd z$\\
	$\dd x$ & $\dd y\wedge \dd z$\\
	$\dd y$ & $\dd z\wedge \dd x$\\
	$\dd z$ & $\dd x\wedge \dd y$
	\end{tabular}
\end{center}

\begin{exercise}
	Verify that corresponding elements in the table are indeed Hodge duals.
\end{exercise}

\subsection{The Hodge Dual and Vector Fields}
With the Hodge dual in hand, we can start nailing down some relationships.

Notice that 1-forms directly correspond to vector fields since covectors directly
correspond to vectors.  We make this correspondence explicit with an operator.
\begin{definition}[Transpose]
	If $\bm\alpha = \sum t_i\bm e^i$ is a covector, then the \emph{transpose}
	of $\bm\alpha$, denoted $\Trans \bm\alpha$, is the vector
	\[
		\Trans \bm\alpha = \sum t_i \vec e_i.
	\]
	If $\vec \alpha=\sum t_i\vec e_i$ is a vector, then the \emph{transpose}
	of $\vec \alpha$, denoted $\Transu\vec \alpha$, is the covector
	\[
		\Transu \vec\alpha = \sum t_i \bm e^i.
	\]
\end{definition}
If you're familiar with the transpose operator from linear algebra, this is
the very same one\footnote{
	If all true vectors are column vectors, then covectors must be row vectors
	and $\bm\alpha(\vec v)$ is just $\bm\alpha\vec v$ where $\bm\alpha$ is a row
	matrix and $\vec v$ is a column matrix.
}.  You might think of it as transposing superscripts into subscripts (since we use
superscripts for covectors and subscripts for vectors).

The transpose is a little like the Hodge dual since $\bm\alpha(\Trans \bm\alpha ) =
\sum t_i^2 = \norm{\Trans \bm\alpha}^2$ and $\Transu\vec \alpha(\vec \alpha) = \sum t_i^2=\norm{\vec \alpha}^2$.

The transpose directly extends to 1-forms and vector fields.  
If $\bm\alpha$ is a 1-form, then $\Trans\bm\alpha$ is a vector field and if $\vec F$
is a vector field, $\Transu \vec F$ is a 1-form.  Further note that $\Transu\Trans \bm\alpha = \bm\alpha$
and $\Trans\Transu\vec\alpha=\vec \alpha$.

It's time to recast some familiar operations in terms of differential forms and the Hodge dual.
\begin{center}
	\begin{tabular}{c|c|c}
	Operation & Notation & Equivalency\\
	\hline
	&&\\[-11pt]
	Cross Product & $\vec u\times \vec v$ & $\Trans \star \big(\!\Transu\vec u\wedge \Transu\vec v \big)$\rule{0pt}{10pt}\\[3pt]
	Gradient & $\nabla f$ & $\Trans \dd (\Transu f)$ \\[3pt]
	Curl & $\nabla \times \vec F$ & $\Trans \star \dd (\Transu \vec F)$\\[3pt]
	Divergence & $\nabla \cdot \vec F$ & $\Trans \star \dd (\star \Transu \vec F)$
	\end{tabular}
\end{center}
Or, if we use bold face for corresponding covectors and forgo some pedanticness,
we have a simplified table.
\begin{center}
	\begin{tabular}{c|c|c}
	Operation & Notation & Equivalency\\
	\hline
	&&\\[-11pt]
	Cross Product & $\vec u\times \vec v$ & $\star \big(\bm u\wedge \bm v \big)$\rule{0pt}{10pt}\\[3pt]
	Gradient & $\nabla f$ & $ \dd \bm f$\\[3pt]
	Curl & $\nabla \times \vec F$ & $\star \dd \bm F$\\[3pt]
	Divergence & $\nabla \cdot \vec F$ & $\star \dd \star \bm F$
	\end{tabular}
\end{center}

Although using ``$\star$'' and ``$\dd$'' we can recover the exact formulations of multivariable
calculus using vector fields, by now there is a strong argument for using differential
forms from the beginning.  Somehow, the fact that covectors and $2$-covectors are duals
of each other in $\R^3$ allows us to get away with calling curl a \emph{vector} rather
than a $2$-covector.  Further, the cross product of two vectors in $\R^3$ is another vector.
We can take the wedge product in any dimensional space, but only in $\R^3$ is $\star(\bm u\wedge \bm v)$
a covector.  The strongest argument yet for formulating multivariable calculus in terms of differential
forms comes from the generalized Stokes's theorem.

\section{Stokes's Theorem}
We've now reached the centerpiece of differential forms---the generalized Stokes's theorem\index{Stokes's theorem}.
And, its statement couldn't be easier.

\begin{theorem}[Generalized Stokes's Theorem]
	Let $\bm\alpha$ be a $k$-form and let $\mathcal R$ be an oriented
	$k$-dimensional region.  Then,
	\[
		\int_{\mathcal R} \dd \bm\alpha = \int_{\partial R} \bm\alpha.
	\]
\end{theorem}

This theorem is equivalent to the fundamental theorem of calculus for 0-forms,
Stokes's curl theorem for 1-forms, and the divergence theorem for 2-forms.  It also
generalizes to all dimensions, and the proof is easy!  We've already seen the main ideas
of the proof.
\begin{enumerate}
	\item Partition $\mathcal R$ by oriented polytopes.  Then, shared faces have opposite
orientations and so cancel.  All that is left are contributions from the boundary.
	\item The definition of $\dd \bm\alpha$ is exactly taking the limit of $\int_B\bm\alpha$
		over smaller and smaller polytopes.  Since these are the same polytopes
		in our partition of $\mathcal R$, we get a $\dd \bm\alpha$ on the left
		and a $\partial \mathcal R$ on the right.
\end{enumerate}
The only hard part of the proof is bookkeeping---making sure the orientation of adjacent
sides really to cancel---and exchanging a limit and a sum---this requires some assumptions
about the differentiability of $\bm\alpha$ and the smoothness of $\partial \mathcal R$.

\section{de Rham Cohomology}

Recall that a vector field $\vec F$ is called \emph{conservative} if
there is a scalar filed $f$ so that $\nabla f=\vec F$.  Further, if $\vec F$ is
conservative, then $\nabla \times \vec F=0$---this was called the screening test.
However, it may be that $\vec F$ is not conservative but even still
$\nabla \times \vec F=0$.  Poincar\'e's lemma states that if $\nabla \times \vec F=0$
on the interior of a ball (equivalently, the interior of a box), then $\vec F$ is conservative.
In other words, if you want a non-conservative vector field to satisfy $\nabla \times\vec F=0$,
you have to make the domain strange.  For example, 
\[
	\vec F(x,y,z) = \frac{1}{x^2+y^2}\mat{-y\\x\\0}
\]
has zero curl, but the $z$-axis must be excluded from the domain.  The key insight here
is that the difference between a conservative and a closed vector field only shows
up when the domain has certain topologies\footnote{ A topology describes how a set is
connected.  A plane and the surface of a doughnut are both two dimensional objects, but they have
very different topologies.}.  Using differential forms, we get a supped up topology-recognizing
machine called \emph{cohomology}\index{cohomology}.

Before we begin, we need some definitions.
\begin{definition}[Closed \& Exact]
	An $n$-form $\bm\alpha$ is called \emph{exact} if there is an
	$(n-1)$-form $\bm\beta$ so that $\bm\alpha=\dd \bm\beta$.  An $n$-form
	$\bm\alpha$ is called \emph{closed} if $\dd \bm\alpha=0$.
\end{definition}
\emph{Exact} is the differential forms term for a conservative vector field.

We can use Stokes's theorem to prove that every exact form is closed.
\begin{theorem}
	Every exact differential form is closed.  That is, $\dd (\dd \bm\alpha)=0$
	for all differential forms $\bm\alpha$.
\end{theorem}
\begin{proof}
	The proof uses Stokes's theorem and topology.  Let $\mathcal R$
	be an oriented region and notice that $\partial (\partial \mathcal R)$ is
	empty---a boundary has no boundary\footnote{
		To formally prove this, we would need to appeal to the definition
		of boundary.  Since $\partial \mathcal R\subseteq \mathcal R$, 
		we know any point $\vec x\in\partial \partial \mathcal R$ is also in $\mathcal R$.
		But, the tangent space to $\mathcal R$ is either $n$-dimension (at points on
		$\mathcal R$'s interior) or $n-1$-dimensional when restricted to $\partial \mathcal R$.
		There's just no room for an $n-2$-dimensional tangent space to fit, so $\vec x\notin\mathcal R$.
		We conclude $\partial\partial \mathcal R$ is empty.
	}.
	From Stokes's theorem we know
	\[
		\int_{\mathcal R}\dd \dd \bm\alpha
		=
		\int_{\partial \mathcal R}\dd \bm\alpha
		=
		\int_{\partial (\partial \mathcal R)}\bm\alpha
		=0
	\]
	since an integral over an empty set is zero.  Any form that integrates to zero over
	all regions must be zero\footnote{ This conclusion requires $\dd \dd\bm\alpha$
	to be continuous, and so $\bm\alpha$ needs to be twice differentiable.}.
\end{proof}

\begin{definition}[Exterior Power]
	Let $\mathcal M$ be a domain (more specifically a \emph{manifold}).
	The set $\Lambda^k(\mathcal M)$ is the set of all $k$-forms on $\mathcal M$
	and is called the \emph{$k$\textsuperscript{th} exterior power of $\mathcal M$}\index{exterior power}.
\end{definition}

Using our new notation, we can say things like $\dd:\Lambda^k(\mathcal M)\to \Lambda^{k+1}(\mathcal M)$.
Further, since $\dd\dd \bm\alpha=0$, we know exact forms are in the kernel of $\dd$.
This allows us to write what (in the abstract algebra) is called an \emph{exact} sequence.
If $\mathcal M$ is $n$-dimensional, then we have the following exact sequence.
\[
	\Lambda^0(\mathcal M) \xrightarrow{\dd}
	\Lambda^1(\mathcal M)\xrightarrow{\dd} \cdots 
	\xrightarrow{\dd} 
	\Lambda^{n-1}(\mathcal M)\xrightarrow{\dd}  
	\Lambda^{n}(\mathcal M)\xrightarrow{\dd}  0
\]
This notation means that the map $\dd$ moves objects from a space on the left
to a space on the right \emph{and} the range of $\dd$ is always contained in the
kernel of $\dd$.  In other words, a chain is called \emph{exact} if two
steps down the chain guarantees you become zero.

If we take $\mathcal M=\R^3$ then we have the explicit exact chain
\[
	\Lambda^0(\R^3) \xrightarrow{\dd}
	\Lambda^1(\R^3)\xrightarrow{\dd} 
	\Lambda^{2}(\R^3)\xrightarrow{\dd}  
	\Lambda^{3}(\R^3)\xrightarrow{\dd}  0.
\]
Further, by Poincar\'e's lemma, the range of $\dd$ is exactly the kernel of the next $\dd$.
This sets our baseline.  Now, consider the space $H=\R^3\backslash\Set{\alpha\xhat\given\alpha\in\R}$
consisting of $\R^3$ without the $z$-axis.  We have a very similar chain.
\[
	\Lambda^0  (H) \xrightarrow{\dd}
	\Lambda^1  (H)\xrightarrow{\dd} 
	\Lambda^{2}(H)\xrightarrow{\dd}  
	\Lambda^{3}(H)\xrightarrow{\dd}  0
\]
Except, this time the range of $\dd$ applied to 0-forms does not equal the kernel
of $\dd$ applied to 1-forms.  This is because there are now covector fields
that circulate around the $z$-axis but still have ``zero curl.''  We won't prove it, but
the image of 1-forms under $\dd$, in this case, is kernel of $\dd$ applied to 2-forms.
This somehow means that some interesting curves fit into $H$, but no interesting surfaces.
The interesting curves are those that circle around the $z$-axis!

We could play this game again.  Let $S=\R^3\backslash\Set{\vec 0}$.  Now we have
a chain
\[
	\Lambda^0  (S) \xrightarrow{\dd}
	\Lambda^1  (S)\xrightarrow{\dd} 
	\Lambda^{2}(S)\xrightarrow{\dd}  
	\Lambda^{3}(S)\xrightarrow{\dd}  0.
\]
In this chain the image of $\dd$ applied to 0-forms is the kernel of $\dd$ applied to 1-forms.
However, the image of $\dd$ applied to 1-forms is \emph{not} the kernel of $\dd$ applied to
2-forms.  Fox example, consider
\[
	\vec F(x,y,z) = \frac{1}{(x^2+y^2+z^2)^{3/2}}\mat{x\\y\\z},
\]
which you may recognize as related the force due to gravity of a point-mass centered
at the origin acting on a particle at position $(x,y,z)$.  $\nabla \cdot \vec F=0$,
but $\vec F$ cannot be the curl of any vector field. Thus, $\bm f=\star \Transu\vec F$ is a
2-form in the kernel of $\dd$ but not the image of any 1-form under $\dd$.  This would be very
hard to show directly from equations, but easy to show with Stokes's theorem.  After all,
$\dd \bm f=0$ is an easy computation,
%\footnote{
%	If you've ever done the computation to show that gravity acting on a sphere is
%	equivalent to gravity acting on a point-mass at the center of the sphere, then
%	you've done this computation.  
%}
but the integral of $\bm f$ around a sphere centered at
the origin and oriented outwards is clearly positive---all vectors coming from $\vec F$ point
radially and symmetrically outwards!

Based on these observations,
we conclude that $S$ cannot contain any loops that are more interesting than those in $\R^3$, but
it can contain surfaces that are more interesting.  What are the interesting surfaces?  The ones
that enclose the origin!
